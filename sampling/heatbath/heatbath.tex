\documentclass[12pt]{article}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}



\begin{document}



\title{Improving acceptance probability}
\author{ \vspace{-10ex} }
\date{ \vspace{-10ex} }
\maketitle


A key step of many Markov chain Monte Carlo methods
is the propose-and-accept scheme.
%
The efficiency depends on the acceptance probability,
or the rate of accepting proposed moves.
%
Two common choices are the Metropolis and heat bath formulas.
%
Here, we wish to discuss some variants
that can deliver higher transition rates.
%
The algorithms may be useful in simulated tempering
and multi-state Monte Carlo sampling problems.


\section{Two-state problem}

Let us start with a simple problem of
sampling a two-state probability distribution.
%
The probability of the two states are $p_1$ and $p_2$,
with $0 \le p_1 \le p_2 \le 1$ and $p_1 + p_2 = 1$.
%
We can do it in two ways.


The first is the heat-bath way:
in every time, we generate a uniform random number $r \in [0, 1]$,
and we pick state 1 if $r \le p_1$, or state 2 otherwise.
%
Keeping doing this yields a Markov chain of states,
and the fraction of instances with state 1 approaches $p_1$.

The second way is the Metropolis way.
We first pick a random state, say $1$.
Then in each step,
we propose to make an attempt of moving to the other state,
and accept it with probability
\begin{equation}
  A(i \rightarrow j) = \min\left\{1, \; \frac{ p_j } { p_i } \right\},
  \label{eq:AMetropolis}
\end{equation}
where $i$ and $j$ are the current and proposed states, respectively.
In our case, a transition from state 1 to state 2 is always accepted,
but the reverse transition is accepted
only with probability $p_1/p_2$.

\section{Transition matrix}

It is easily seen that
the Metropolis way gives larger transition probabilities,
and hence is more efficient.
%
In the heat-bath way, the transition matrix is
$$
\mathbf T_h =
\left(
  \begin{array}{ccc}
    p_1 & p_1 \\
    p_2 & p_2
  \end{array}
\right),
$$
where the row and column indices represent final and initial states,
respectively.
But in the Metropolis way, the transition matrix reads
$$
\mathbf T_M =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{ccc}
    0 & \dfrac{ p_1 }{ p_2 }  \\
    1 & 1 - \dfrac{ p_1 }{ p_2 }
  \end{array}
\right).
$$
%
The off-diagonal elements of the transition matrix represent
probabilities of successful transitions;
the diagonal elements those of unsuccessful ones.
%
So we wish to maximize the former and minimize the latter.


Now since $1 > p_2$ for the first column,
and $p_1/p_2 > p_1$ for the second column,
we conclude that the Metropolis way is more efficient.
%
In fact, we can show that in the two-state case,
the Metropolis way is the \emph{most} efficient way
in terms of delivering the largest transition rates.

The last statement has a somewhat counterintuitive consequence.
If we combine two or more Metropolis transitions
in a single step, we can only achieve an inferior
sampling scheme with lower success rates.
The reason is that subsequent transitions tend to
undo a good transition made in the first attempt,
and hence increase the rate of rejection.




\section{Multiple-state case}

The above analysis shows the superiority of the Metropolis way
over the heat-bath one in the two-state case.
%
Unfortunately, when we have multiple states,
the straightforward generalization of the Metropolis way
is often inferior to the heat-bath way.
%
The reason is that when there are many options with
a nonuniform probability distribution,
the heat-bath way can more readily identify the dominant options.
%
We shall illustrate the feature below.
%
But first, let us review basic properties of the transition matrix.


\section{Properties of the transition matrix}

Let us review a few properties of the transition matrix.
%
\begin{enumerate}
  \item Any element of the matrix is between $0$ and $1$,
    because it is a probability.
  \item The sum of any column is $1$, because it
    sums over the total probabilities to all destinations.
  \item The stationary distribution, arranged in a column vector,
      $\mathbf p^* = (p_1, \dots, p_n)^T$ is an eigenvector of the matrix
      with eigenvalue $\lambda_1 = 1$. That is,
      \begin{equation}
        \mathbf T \mathbf p^* = \mathbf p^*.
        \label{eq:balance}
      \end{equation}
      %
      This means the transition matrix leaves
      the stationary distribution invariant.
      This condition is often called the \emph{balance} condition.
  \item
      In most cases, $\mathbf p^*$ is the only eigenvector
      with eigenvalue $\lambda_1 = 1$.
      All other eigenvectors have eigenvalues $\lambda \in [-1, 1)$
        or complex values with $|\lambda| \le 1$.
  \item
      Particularly, the second largest eigenvalue $\lambda_2$ can be used to represent
      the overall rate of transition (the smaller the better).
\end{enumerate}
%
These properties are satisfied by both transition matrices
in the above example.

\paragraph*{Detailed balance.}

In practice, we often use a stronger condition than Eq. \eqref{eq:balance}.
\begin{equation}
  T_{ij} \, p_j = T_{ji} \, p_i.
  \label{eq:detailedbalance}
\end{equation}
This condition is called the \emph{detailed balance} condition,
which implies the balance condition Eq. \eqref{eq:balance}:
$$
\sum_j T_{ij} p_j = \sum_j T_{ji} p_i = p_i.
$$
However, detailed balance is not a necessary condition.


\section{\label{sec:heatbath}
The heat-bath algorithm}

In the multiple state case,
a step of the heat-bath way is done as follows.
%
\begin{enumerate}
  \item Compute the cumulative distribution function,
    $$
    P_k = \sum_{i=1}^k p_i = P_{k-1} + p_k,
    $$
    with $P_0 = 0$.
  \item Generate a uniform random number $r \in [0, 1]$.
  \item Find the $i$ such that $P_{i-1} \le r < P_i$,
    and choose state $i$.
\end{enumerate}
Thus, the rate of selecting state $i$
is given by $p_i$, independent of the current state.

The transition matrix is
$$
\mathbf T_h =
\left(
  \begin{array}{cccccc}
    p_1 & p_1 & \dots & p_1 \\
    p_2 & p_2 & \dots & p_2 \\
    \vdots & \vdots  &  & \vdots \\
    p_n & p_n & \dots & p_n
  \end{array}
\right).
$$
It can be shown that
the largest eigenvalue is $1$, the rest of the eigenvalues are $0$
because it contains only one independent column (rank 1).
%
The diagonal elements represent rejections,
thus the average rejection ratio is given by
\begin{equation}
  R_h = \sum_{i = 1}^n p_i^2 = \overline { p_i }.
\label{eq:rejection_heatbath}
\end{equation}
If the distribution is uniform, then $R_h = 1/n$.
But if the distribution is dominated by $p_n = 1 - \delta$
(with $\delta \ll 1$),
then
\begin{equation}
  R_h \approx p_n^2 \approx 1 - 2 \, \delta.
\label{eq:rejection_heatbath_domlimit}
\end{equation}




\section{\label{sec:Metropolis}
The Metropolis algorithm}


The straightforward generalization of the Metropolis way is the following.
We randomly choose a state $j$ that is not the current one $i$,
and use Eq. \eqref{eq:AMetropolis} to accept it.

The transition matrix is
$$
\mathbf T_M =
\left(
  \arraycolsep=8.0pt\def\arraystretch{2.0}
  \begin{array}{cccccc}
    0   & \dfrac{1}{n-1}\dfrac{p_1}{p_2} & \dots & \dfrac{1}{n-1}\dfrac{p_1}{p_n} \\
    \dfrac{1}{n-1} & \dfrac{1}{n-1}\dfrac{p_2-p_1}{p_2} & \dots & \dfrac{1}{n-1}\dfrac{p_2}{p_n} \\
    \vdots & \vdots  &  & \vdots \\
    \dfrac{1}{n-1} & \dfrac{1}{n-1} & \dots & 1 - \dfrac{1}{n-1}\dfrac{p_1 + \cdots + p_{n-1}}{p_n}
  \end{array}
\right).
$$
We can compute the average rejection rate\footnote{
\begin{align*}
  R_M
  &= \sum_{i = 2}^n p_i \sum_{j = 1}^{i-1} \frac{1}{n-1} \frac{p_i - p_j} { p_i }
  = \frac{1}{n-1} \sum_{1 \le j < i \le n} (p_i - p_j)
  \notag \\
  &= \frac{1}{n-1} \left( \sum_{i=1}^n (i - 1) \, p_i - \sum_{j = 1}^{n-1} (n - j) \, p_j \right)
  = \frac{2}{n-1} \sum_{i=1}^n i \, p_i - \frac{n+1}{n-1}.
\end{align*}
}
\begin{align}
  R_M
  &= \frac{2 \sum_{i=1}^n i \, p_i - (n + 1) }{n-1}
  = \frac{ 2 \, \overline i - (n + 1) } { n - 1 }.
\label{eq:rejection_Metropolis}
\end{align}
For a uniform distribution, $p_i = 1/n$, then $R_M = 0$
(better than the heat-bath case).
But if the distribution is dominated by
the last entry $p_n = 1 - \delta$
with ($\delta \ll 1$),
then\footnote{
Consider two limits.
The first is $p_1 = \dots = p_{n-1} = (1-\delta)/(n-1)$.
$$
R_M = \frac{2}{n-1}
\left( n (1 - \delta) + \frac{n(n-1)}{2} \frac{\delta}{n-1} \right)
- \frac{ n + 1 } { n - 1 } = 1 - \frac{n}{n-1}\delta.
$$
The other limit is $p_{n-1} = \delta$, $p_1 = \dots = p_{n-2} = 0$.
$$
R_M = \frac{2}{n-1}
\left( n (1 - \delta) + (n-1)\delta \right)
- \frac{ n + 1 } { n - 1 } = 1 - \frac{2}{n-1}\delta,
$$
which is even worse.
}
$$
R_M \ge 1 - \frac{n}{n-1}\delta.
$$
which is worse than the heat-bath case for $n \ge 3$.
%



\section{\label{sec:convective}
Convective Metropolis algorithm}



So the question is that if there is a way to generalize
the Metropolis way without loss of the advantage in the two-state case.
%
The answer is yes.

We assume that $p_1 \le \dots \le p_n$ as before.
We will show below that if $p_1 + \dots + p_{n-1} \ge p_n$,
it is possible to design a rejection-free transition scheme.
Even for $p_1 + \dots + p_{n-1} < p_n$,
our scheme will deliver a high success rate
that the heat-bath algorithm.

We consider several cases.

\subsection{Type I.}

\paragraph{Case 1.}

First, if $p_1 \le p_n \le p_1+ p_2$,
consider the matrix
$$
  \mathbf T_{c}
=
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{cccccc}
    0     &   0   &   0   &   \dots  &   \dfrac{p_1}{p_n}       \\
    1-\dfrac{p_n - p_2}{p_1}   &   0   &   0   &   \dots  &   1 - \dfrac{p_1}{p_n} \\
    \dfrac{p_3 - p_2}{p_1}     &   1   &   0   &   \dots  &   0 \\
    \dfrac{p_4 - p_3}{p_1}     &   0   &   1   &   \dots  &   0 \\
    \vdots&\vdots &\vdots &   \ddots &   \vdots \\
    \dfrac{p_n - p_{n-1}}{p_1} &   0   &   0   &   \dots  &   0 \\
  \end{array}
\right).
$$
It is readily verified that $(p_1, \dots, p_n)^T$
is a right eigenvector with eigenvalue $1$.
%
So the stationary distribution is preserved.
%
But more interestingly,
the diagonal elements of the matrix are all zeros,
meaning that the new scheme is rejection-free!
%
However, it only works for $p_1 + p_2 \ge p_n$,
for otherwise the element $T_c(2,1)$ is negative.

\paragraph{Case 2.}

Second, if $p_1 + p_2 \le p_n \le p_1 + p_2 + p_3$,
we give
$$
  \mathbf T_{c}
=
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{cccccc}
    0     &   0   &   0   &   \dots  &   \dfrac{p_1}{p_n}       \\
    0     &   0   &   0   &   \dots  &   \dfrac{p_2}{p_n} \\
    1 - \dfrac{p_n - p_3}{p_1 + p_2}  &   1 - \dfrac{p_n - p_3}{p_1 + p_2}  &   0   &   \dots  &   1 - \dfrac{p_1+p_2}{p_n} \\
    \dfrac{p_4 - p_3}{p_1 + p_2}     &  \dfrac{p_4 - p_3}{p_1 + p_2}   &   1   &   \dots  &   0 \\
    \vdots&\vdots &\vdots &   \ddots &   \vdots \\
    \dfrac{p_n - p_{n-1}}{p_1 + p_2} &  \dfrac{p_n - p_{n-1}}{p_1 + p_2} &   0   &   \dots  &   0 \\
  \end{array}
\right).
$$
Again, the stationary distribution is preserved,
and the diagonal elements of the matrix are zeros,
meaning a rejection-free scheme.

\paragraph{Case 3.}

We give one more example, just to make the pattern clear.
%
For $n \ge 5$, and
if $p_1 + p_2 + p_3 \le p_n \le p_1 + p_2 + p_3 + p_4$,
then
$$
  \mathbf T_{c}
=
\left(
  \arraycolsep=7.0pt\def\arraystretch{1.5}
  \begin{array}{cccccccc}
    0     &   0   &   0   &   0 & \dots  &   \frac{p_1}{p_n}       \\
    0     &   0   &   0   &   0 & \dots  &   \frac{p_2}{p_n} \\
    0     &   0   &   0   &   0 & \dots  &   \frac{p_3}{p_n} \\
    1 - \frac{p_n - p_4}{p_1 + p_2 + p_3}  &
    1 - \frac{p_n - p_4}{p_1 + p_2 + p_3}  &
    1 - \frac{p_n - p_4}{p_1 + p_2 + p_3}  & 0   &   \dots  &   1 - \frac{p_1+p_2 + p_3}{p_n} \\
    \frac{p_5 - p_4}{p_1 + p_2 + p_3}   &
    \frac{p_5 - p_4}{p_1 + p_2 + p_3}   &
    \frac{p_5 - p_4}{p_1 + p_2 + p_3}   &
    1   &   \dots  &   0 \\
    \vdots&\vdots &\vdots & \vdots  &   \vdots \\
    \frac{p_n - p_{n-1}}{p_1 + p_2 + p_3} &
    \frac{p_n - p_{n-1}}{p_1 + p_2 + p_3} &
    \frac{p_n - p_{n-1}}{p_1 + p_2 + p_3} &
    0   &   \dots  &   0 \\
  \end{array}
\right).
$$

\paragraph{Case $n$.}

However, if $p_n > p_1 + \dots + p_{n-1}$, we no longer
have a rejection free matrix.  The best we can get is
\begin{equation}
  \mathbf T_{c}
=
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{cccccc}
    0     &   0   &   \dots  &   \dfrac{p_1}{p_n}       \\
    0     &   0   &   \dots  &   \dfrac{p_2}{p_n} \\
    \vdots&\vdots &   \ddots &   \vdots \\
    1 &  1 &   \dots  &   1 - \dfrac{p_1 + \cdots + p_{n-1}}{p_n} \\
  \end{array}
\right).
\label{eq:T_convective_n}
\end{equation}
Even in this case,
the rejection rate is
\begin{equation}
R_c = p_n - (p_1 + \dots + p_{n-1}) = 2 p_n - 1,
\label{eq:rejection_convective}
\end{equation}
which is always lower than the rate
in the heat-bath case,
Eq. \eqref{eq:rejection_heatbath}.
\footnote{
  Because
  $(p_1^2 + \dots + p_n^2) -  (2 \, p_n - 1)
  = p_1^2 + \dots + p_{n-1}^2 + (1 - p_n)^2 \ge 0$.
}

\subsection{Type II.}

Interestingly, the rejection-free scheme are not unique.

\paragraph{Case 1.}

If $p_n \le p_{n-1} + p_{n-2}$.
$$
  \mathbf T_{c}
=
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{ccccccc}
    0  & \dfrac{p_1}{p_2}   &   0   &   \dots  & 0  & 0 \\
    0  &   0   & \dfrac{p_2}{p_3}  &   \dots  &  0 & 0 \\
    \vdots&\vdots &\vdots &   \ddots &   \vdots & \vdots\\
    0 & 0 &0 &   \cdots &   1 - \dfrac{ p_n - p_{n-2} } {p_{n-1}} & 1 - \dfrac{p_{n-1}}{p_n}\\
    0 & 0 &0 &   \cdots &  0 & \dfrac{p_{n-1}}{p_n}\\
    1 &  1 - \dfrac{p_1}{p_2} &   1 - \dfrac{p_2}{p_3}   &   \dots  &  \dfrac{p_n - p_{n-2}}{p_{n-1}} & 0
  \end{array}
\right).
$$

\paragraph{Case 2.}

If $p_{n-1} + p_{n-2} < p_n \le p_{n-1} + p_{n-2} + p_{n-3}$.
$$
  \mathbf T_{c}
=
\left(
  \arraycolsep=6.0pt\def\arraystretch{1.5}
  \begin{array}{ccccccc}
    0  & \frac{p_1}{p_2}   &   \dots  & 0  & 0 & 0\\
    0  &   0   &   \dots  &  0 & 0 & 0\\
    \vdots&\vdots &   \ddots &   \vdots & \vdots & \vdots \\
    0 & 0 &   \cdots &
    1 - \frac{ p_n - p_{n-3} } { p_{n-2} + p_{n-1}} &
    1 - \frac{ p_n - p_{n-3} } { p_{n-2} + p_{n-1}} &
    1 - \frac{p_{n-2} + p_{n-1}}{p_n}\\
    0 & 0 &   \cdots &  0 & 0 & \frac{p_{n-2}}{p_n}\\
    0 & 0 &   \cdots &  0 & 0 & \frac{p_{n-1}}{p_n}\\
    1 &  1 - \frac{p_1}{p_2} &   \dots  &
    \frac{p_n - p_{n-3}}{p_{n-2} + p_{n-1}} &
    \frac{p_n - p_{n-3}}{p_{n-2} + p_{n-1}} & 0
  \end{array}
\right).
$$

The trend is clear.

\paragraph{Case $n$.}

If $p_n > p_1 + \cdots + p_{n-1}$,
the transition matrix coincides
with the one given by Eq. \eqref{eq:T_convective_n}



\end{document}
