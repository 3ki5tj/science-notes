\documentclass[12pt]{article}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}



\begin{document}



\title{Improving acceptance probability}
\author{ \vspace{-10ex} }
\date{ \vspace{-10ex} }
\maketitle


A key step of many Markov chain Monte Carlo methods,
is the propose-and-accept scheme.
%
The efficiency depends the acceptance probability
for the frequency of accepting proposed moves.
%
Here, we wish to discuss some variants of
the commonly used Metropolis algorithm,
and the heat-bath algorithm,
which can deliver higher transition rates.
%
This algorithm may be useful in simulated tempering
and multi-state Monte Carlo sampling problems.


\section{Two-state problem}

Let us start with a simple problem of
sampling a two-state probability distribution.
%
The probability of the two states are $p_1$ and $p_2$,
with $0 \le p_1 \le p_2 \le 1$ and $p_1 + p_2 = 1$.
%
We can do it in two ways.


The first is the heat-bath way:
in every time, we generate a uniform random number $r \in [0, 1]$,
and we pick state 1 if $r \le p_1$, or state 2 otherwise.
%
Keeping doing this yields a Markov chain of states,
and the fraction of instances with state 1 approaches $p_1$.

The second way is the Metropolis way.
We first pick a random state, say $1$.
Then in each step,
we propose to make an attempt of moving to the other state,
and accept it with probability
\begin{equation}
  A(i \rightarrow j) = \min\left\{1, \; \frac{ p_j } { p_i } \right\},
  \label{eq:AMetropolis}
\end{equation}
where $i$ and $j$ are the current and proposed states, respectively.
In our case, a transition from state 1 to state 2 is always accepted,
but the reverse transition is accepted
only with probability $p_1/p_2$.

\section{Transition matrix}

It is easily seen that
the Metropolis way gives larger transition probabilities,
and hence is more efficient.
%
In the heat-bath way, the transition matrix is
$$
\mathbf T_h =
\left(
  \begin{array}{ccc}
    p_1 & p_1 \\
    p_2 & p_2
  \end{array}
\right),
$$
where the row and column indices represent final and initial states,
respectively.
But in the Metropolis way, the transition matrix reads
$$
\mathbf T_M =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{ccc}
    0 & \dfrac{ p_1 }{ p_2 }  \\
    1 & 1 - \dfrac{ p_1 }{ p_2 }
  \end{array}
\right).
$$
%
The off-diagonal elements of the transition matrix represent
probabilities of successful transitions;
the diagonal elements those of unsuccessful ones.
%
So we wish to maximize the former and minimize the latter.


Now since $1 > p_2$ for the first column,
and $p_1/p_2 > p_1$ for the second column,
we conclude that the Metropolis way is more efficient.
%
In fact, we can show that in the two-state case,
the Metropolis way is the \emph{most} efficient way
in terms of delivering the largest transition rates.

The last statement has a somewhat counterintuitive consequence.
If we combine two or more Metropolis transitions
in a single step, we can only achieve an inferior
sampling scheme with smaller successful transition rates.
The reason is that subsequent transitions tend to
undo a good transition made in the first attempt,
and hence increase the rate of rejection.




\section{Multiple-state case}

The above analysis shows the superiority of the Metropolis way
over the heat-bath one in the two-state case.
%
Unfortunately, when we have multiple states,
the straightforward generalization of the Metropolis way
is often inferior to the heat-bath way.
%
The reason is that when there are many options with
a nonuniform probability distribution,
the heat-bath way can more readily identify the dominant options.
%
We shall illustrate the feature below.
%
But first, let us review a few properties of the transition matrix.


\section{Properties of the transition matrix}

Let us review a few properties of the transition matrix.
%
\begin{enumerate}
  \item Any element of the matrix is between $0$ and $1$,
    because it is a probability.
  \item The sum of any column is $1$, because it
    sums over the total probabilities to all destinations.
  \item The stationary distribution, arranged in a column vector,
      $\mathbf p^* = (p_1, \dots, p_n)^T$ is an eigenvector of the matrix
      with eigenvalue $\lambda_1 = 1$. That is,
      \begin{equation}
        \mathbf T \mathbf p^* = \mathbf p^*.
        \label{eq:balance}
      \end{equation}
      %
      This means the transition matrix leaves
      the stationary distribution invariant.
      This condition is often called the balance condition.
  \item
      In most cases, $\mathbf p^*$ is the only eigenvector
      with eigenvalue $\lambda_1 = 1$.
      All other eigenvectors have eigenvalues $\lambda \in [-1, 1)$
        or complex values with $|\lambda| \le 1$.
  \item
      Particularly, the second largest eigenvalue $\lambda_2$ can be used to represent
      the overall rate of transition (the smaller the better).
\end{enumerate}
%
These properties are satisfied by both transition matrices
in the above example.

\paragraph*{Detailed balance.}

In practice, we often use a stronger condition than Eq. \eqref{eq:balance}.
\begin{equation}
  T_{ij} \, p_j = T_{ji} \, p_i.
  \label{eq:detailedbalance}
\end{equation}
This condition is called detailed balance,
which implies the balance condition Eq. \eqref{eq:balance}:
$$
\sum_j T_{ij} p_j = \sum_j T_{ji} p_i = p_i.
$$
However, detailed balance is not a necessary condition.


\section{The heat-bath algorithm}

In the multiple state case,
a step of the heat-bath way is done as follows.
%
\begin{enumerate}
  \item Compute the cumulative distribution function,
    $$
    P_k = \sum_{i=1}^k p_i = P_{k-1} + p_k,
    $$
    with $P_0 = 0$.
  \item Generate a uniform random number $r \in [0, 1]$.
  \item Find the $i$ such that $P_{i-1} \le r < P_i$,
    and choose state $i$.
\end{enumerate}

The transition matrix is
$$
\mathbf T_h =
\left(
  \begin{array}{cccccc}
    p_1 & p_1 & \dots & p_1 \\
    p_2 & p_2 & \dots & p_2 \\
    \vdots & \vdots  &  & \vdots \\
    p_n & p_n & \dots & p_n
  \end{array}
\right).
$$
It can be shown that
the largest eigenvalue is $1$, the rest of the eigenvalues are $0$
because it contains only one independent column (rank 1).



\section{The Metropolis algorithm}


The straightforward generalization of the Metropolis way is the following.
We randomly choose a state $j$ that is not the current one $i$,
and the Metropolis rule Eq. \eqref{eq:AMetropolis} to accept it.

The transition matrix is
$$
\mathbf T_M =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.0}
  \begin{array}{cccccc}
    0   & \frac{1}{n-1}\frac{p_1}{p_2} & \dots & \frac{1}{n-1}\frac{p_1}{p_n} \\
    \frac{1}{n-1} & \frac{1}{n-1}\frac{p_2-p_1}{p_2} & \dots & \frac{1}{n-1}\frac{p_2}{p_n} \\
    \vdots & \vdots  &  & \vdots \\
    \frac{1}{n-1} & \frac{1}{n-1} & \dots & 1 - \frac{1}{n-1}\frac{p_1 + \cdots + p_{n-1}}{p_n}
  \end{array}
\right).
$$

To compare this matrix with the heat-bath one.
Let us consider the three-state case
$$
\mathbf T_M =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.5}
  \begin{array}{cccccc}
    0   & \dfrac{p_1}{2 \, p_2} & \dfrac{p_1}{2 \, p_3} \\
    \dfrac{1}{2} & \dfrac{p_2-p_1}{2 \, p_2} & \dfrac{p_2}{2 \, p_3} \\
    \dfrac{1}{2} & \dfrac{1}{2} & 1 - \dfrac{p_1 + p_{2}}{2 \, p_3}
  \end{array}
\right).
$$
The three eigenvalues are
$1$, $1 - \frac{1}{2 p_3}$, and $-\frac{p_1}{2 p_2}$.
%
Now if $p_3 > \frac{1}{2}$, the second largest eigenvalue
is greater than zero, meaning that the Metropolis algorithm
is less efficient than the heat-bath algorithm.
For more states, the heat-bath will gain further advantage.
\footnote{
Generally, one can show that
the second largest eigenvalue is $1- \frac{1}{(n-1)p_n}$,
with the eigenvector being
$(-p_1, \cdots, -p_{n-1}, p_1 + \cdots + p_{n-1})^T$,
which represents a mode of entering or leaving the last state.
This means as long as the largest probability $p_n > 1/(n-1)$,
the Metropolis way is always inferior to the heat-bath way!
}



\section{Improved transition matrix (with detailed balance)}



So the question is that if there is a way to generalize
the Metropolis way without loss of the advantage in the two-state case.
%
The answer is yes.
%
Let us work on the three-state case.
%
For the first column we shall demand that $T_{1,1} = 0$
and let the rest of the cells be proportional to
the stationary distribution. That is
$$
\mathbf T =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.5}
  \begin{array}{cccccc}
    0   &  X & X \\
    \dfrac{p_2}{p_2+p_3} &  & \\
    \dfrac{p_3}{p_2+p_3} &  &
  \end{array}
\right).
$$
Next by detailed balance, we can determine the first row.
For $T_{1,2}$,
we have $T_{1,2} \, p_2 = T_{2,1} \, p_1$,
and $T_{1,2} = p_1/(p_2 + p_3)$.
$$
\mathbf T =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.5}
  \begin{array}{cccccc}
    0   &  \dfrac{p_1}{p_2+p_3} & \dfrac{p_1}{p_2+p_3} \\
    \dfrac{p_2}{p_2+p_3} &  & \\
    \dfrac{p_3}{p_2+p_3} &  &
  \end{array}
\right).
$$
But now we are facing a similar problem to the previous one,
but for the $2\times 2$ submatrix on the lower-right corner.
Again, if we assume $T_{2,2} = 0$, and define $q_2 = 1- p_1/(p_2 + p_3)$,
we have
$$
\mathbf T =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.5}
  \begin{array}{cccccc}
    0   &  \dfrac{p_1}{p_2+p_3} & \dfrac{p_1}{p_2+p_3} \\
    \dfrac{p_2}{p_2+p_3} &  0 & \\
    \dfrac{p_3}{p_2+p_3} &  q_2 &
  \end{array}
\right).
$$
and by detailed balance, we get
$$
\mathbf T =
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.5}
  \begin{array}{cccccc}
    0   &  \dfrac{p_1}{p_2+p_3} & \dfrac{p_1}{p_2+p_3} \\
    \dfrac{p_2}{p_2+p_3} &  0 & q_2 \dfrac{p_2}{p_3} \\
    \dfrac{p_3}{p_2+p_3} &  q_2 & q_3
  \end{array}
\right).
$$
The last cell has been filled by $q_3 = 1 - T_{3,1} - T_{3,2} = q_2 (1 - p_2/p_3)$,
according to the normalization.

This matrix has two zeros on the diagonal.
So we expect it to be more efficient than the heat-bath matrix.
Indeed we can show that it has three eigenvalues
\begin{align}
  \lambda_1 &= 1 \\
  \lambda_2 &= -\frac{p_1}{p_2+p_3} \\
  \lambda_3 &= -\frac{p_2}{p_3}\left( 1 - \frac{p_1}{p_2+p_3} \right)
  \label{eq:dbimproved}
\end{align}
in descending order.
Thus, the second largest eigenvalue is less than zero,
improving the heat-bath result.

\paragraph{General case.}

Generally, we give the transition matrix
$$
T_{ij} =
\begin{cases}
  0 & \mbox{if $i = j < n$} \\
  q_n & \mbox{if $i = j = n$} \\
  \dfrac{p_i \, q_i } { p_{i+1} + \cdots + p_n} & \mbox{if $i < j$} \\
  \dfrac{p_i \, q_j}{p_{j+1} + \cdots + p_n} & \mbox{if $i > j$}
\end{cases}
$$
where $q_j$ is defined recursively defined as
$$
\begin{aligned}
  q_{i+1} &\equiv \left(1 - \frac{p_i}{p_{i+1} + \cdots + p_n} \right) q_i,
\end{aligned}
$$
with $q_1 = 1$.

It is readily shown that detailed balance Eq. \eqref{eq:detailedbalance} is satisfied.
Further we can show that the second largest eigenvalue
is given by $\lambda_2 = -p_1/(p_2 + \cdots + p_n)$,
corresponding to the eigenvector
$\mathbf v = (-p_2 - \cdots - p_n, p_2, \cdots, p_n)^T$.
\footnote{
The proof is given below. We want to show
$$
\sum_{j = 1}^n T_{ij} \, v_j = \lambda_2 \, v_i.
$$
For $i = 1$,
$$
\sum_{j=1}^n T_{1j} v_j =
\frac{ p_1 } { p_2 + \cdots + p_n } \sum_{j=2}^n p_j
= p_1
= \lambda_2 v_1.
$$
For $i = 2, \dots, n-1$,
$$
\begin{aligned}
\sum_{j=1}^n T_{ij} v_j
&=
-\frac{ p_i \, q_1 \, (p_2 + \cdots + p_n) } { p_{2} + \cdots + p_n }
+p_i \sum_{j = 2}^{i-1} \frac{ q_j \, p_j } { p_{j+1} + \cdots + p_n }
+\frac{ p_i \, q_i } { p_{i+1} + \cdots + p_n } \sum_{j=i+1}^n p_j \\
&=
-p_i \, q_1
+ p_i \sum_{j = 2}^{i-1} (q_j - q_{j+1})
+p_i \, q_i
=p_i (q_2 - q_1) = p_i \, \lambda_2 = \lambda_2 \, v_i.
\end{aligned}
$$
For $i = n$,
$$
\begin{aligned}
\sum_{j=1}^n T_{nj} v_j
&=
-\frac{ p_n \, q_1 \, (p_2 + \cdots + p_n) } { p_{2} + \cdots + p_n }
+p_n \sum_{j = 2}^{n-1} \frac{ q_j \, p_j } { p_{j+1} + \cdots + p_n }
+q_n \, p_n \\
&=
-p_n \, q_1
+p_n \sum_{j = 2}^{n-1} (q_j - q_{j+1})
+q_n \, p_n
=p_n \, (q_2 - q_1) = p_n \, \lambda_2 = \lambda_2 \, v_n.
\end{aligned}
$$
}

This scheme is indeed better.
But if $p_1$ is small,
the improvement over the heat-bath algorithm is minimal,
since $\lambda_2$ would be very close to $0$.
So can we do better?
%
Below we show that
if we are willing to give up detailed balance,
this can indeed be done.



\section{Improved transition matrix (without detailed balance)}


Let us consider the three-state case.
$$
\mathbf T_c
=
\left(
  \arraycolsep=10.0pt\def\arraystretch{2.5}
  \begin{array}{ccc}
    0     &   0   &   \dfrac{p_1}{p_3}   \\
    1     &   0   &   \dfrac{p_2 - p_1}{p_3} \\
    0     &   1   &   1 - \dfrac{p_2}{p_3}
  \end{array}
\right)
$$
It is readily verified that this matrix satisfies balance.
That is, $\lambda_1 = 1$ with eigenvector $(p_1, p_2, p_3)^T$.
The other two eigenvalues are
$$
\lambda_{2,3}
=
\frac{-p_2 \pm \sqrt{p_2^2 - 4 \, p_1 \, p_3} } { 2 \, p_3 }.
$$

However, this matrix does not satisfy detailed balance,
$$
p_1 = T_{21} p_1 \ne T_{12} p_2 = 0.
$$


This formula is interesting.
Let us first consider the case of $p_2^2 \ge 4 \, p_1 \, p_3$,
then
$$
\lambda_2 =
\frac{-p_2 + \sqrt{p_2^2 - 4 \, p_1 \, p_3} } { 2 \, p_3 }
=
-\frac{2 \, p_1 }{p_2 + \sqrt{p_2^2 - 4 \, p_1 \, p_3} },
$$
which is less than the value given by Eq. \eqref{eq:dbimproved},
showing improvements.

But more interestingly, if $p_2^2 \le 4 \, p_1 \, p_3$
the eigenvalues are complex!
$$
\lambda_{2,3}
=
\frac{-p_2 \pm \sqrt{4 \, p_1 \, p_3 - p_2^2} \, i } { 2 \, p_3 },
$$
and $\|\lambda_{2,3}\| = p_1/p_3$.
%
Particularly, if $p_1 = p_2 = p_3$, we get
$$
\mathbf T_c
=
\left(
  \begin{array}{ccc}
    0     &   0   &   1   \\
    1     &   0   &   0 \\
    0     &   1   &   0
  \end{array}
\right)
$$
which represents a deterministic convective flow
$1\rightarrow 2 \rightarrow 3 \rightarrow 1$.
Thus, this convective way can more uniformly
sample the desired distribution by reducing fluctuations.

The general case is
$$
\mathbf T_c
=
\left(
  \arraycolsep=15.0pt\def\arraystretch{2.5}
  \begin{array}{cccccc}
    0     &   0   &   0   &   \dots  &   \dfrac{p_1}{p_n}       \\
    1     &   0   &   0   &   \dots  &   \dfrac{p_2 - p_1}{p_n} \\
    0     &   1   &   0   &   \dots  &   \dfrac{p_3 - p_2}{p_n} \\
    \vdots&\vdots &\vdots &   \ddots &   \vdots \\
    0     &   0   &   0   &   \dots  &   1- \dfrac{p_{n-1}}{p_n} \\
  \end{array}
\right).
$$


\end{document}
