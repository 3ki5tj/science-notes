\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    linkcolor={red!30!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


\begin{document}


\title{Notes on Stochastic Calculus}
\author{ \vspace{-10ex} }
\date{ \vspace{-10ex} }
\maketitle

\section{Computing expectation from the CDF}

The cumulative distribution function (CDF) is defined as
$$
F_X(x) \equiv \mathrm{Pr}(X \le x).
$$
The complementary CDF is defined as
$$
F^c_X(x) \equiv \mathrm{Pr}(X \ge x) = 1 - F_X(x).
$$

There is an interesting theorem of computing the average
from the complementary CDF:
\begin{align}
\mathrm{E}[X] &= \int_0^\infty \mathrm{Pr}(X \ge x) \, dx.
\label{eq:avccdf}
\end{align}

Proof. Since the probability density is
$$
p_X(x) = - \frac{ d \mathrm{Pr}(X\ge x) }{ d x }.
$$
So the expected value
$$
\begin{aligned}
\mathrm{E}[X] &= \int_0^\infty x \, p_X(x) \, dx
 = -\int_0^\infty x \, d \mathrm{Pr}(X \ge x) \\
 &= -x \, \mathrm{Pr}(X\ge x) \Bigr|_0^\infty
  + \int_0^\infty \mathrm{Pr}(X \ge x) \, dx \\
 &= \int_0^\infty \mathrm{Pr}(X \ge x) \, dx.
\end{aligned}
$$
Geometrically, it means that the expectation can be computed from
the area under the curve of $F^c_X(x) = \mathrm{Pr}(X \ge x)$.


\section{Probability inequalities}

[From Robert Gallager's third lecture on stochastic process,
YouTube link: \url{https://www.youtube.com/watch?v=k0UZNZwPO8Q}.

Lecture notes: \url{http://www.rle.mit.edu/rgallager/documents/6.262lateweb1.pdf}.

See also \url{http://onlinelibrary.wiley.com/doi/10.1002/9781118593103.app2/pdf}.]



\subsection{Markov's inequality}

For a nonnegative random variable $y$, Markov's inequality 
\begin{equation}
\mathrm{Pr}(Y\ge y) \le \frac{ \mathrm{E}(Y) } { y }.
\label{eq:markov_eq}
\end{equation}

Proof.
From Eq. \eqref{eq:avccdf}, we get
$$
\begin{aligned}
\mathrm{E}[Y] = \int_0^\infty \mathrm{Pr}(Y \ge y) \, dy.
\end{aligned}
$$
This means that $E[Y]$ is the area under the curve $\mathrm{Pr}(Y\ge y)$,
which is a function decreasing from $1$ at $y = 0$ to $0$ at $y = \infty$.
%
Now $\mathrm{Pr}(Y\ge y)$ is the area of the rectangle from $(0, 0)$
to $(y, \mathrm{Pr}(Y\ge y))$.
This area is obviously no greater than the area represented by $\mathrm{E}[Y]$.
So
%
$$
y \, \mathrm{Pr}(Y \ge y) \le \mathrm{E}[Y],
$$
as is to be shown.


\subsection{Chebyshev's inequality}

If $Z$ has a mean $\mathrm{E}[Z] = \overline Z$
and a variance, $\sigma_Z^2$, then for any $\epsilon > 0$,
we have Chebyshev's inequality.
\begin{equation}
\mathrm{Pr}\Bigl\{\bigl|Z - \overline Z \bigr|\ge \epsilon\Bigr\} \le \frac{\sigma_Z^2}{\epsilon^2}.
\label{eq:chebshev_ineq}
\end{equation}

This can be shown by using Markov's inequality with $Y = (Z - \overline Z)^2$
$$
\mathrm{Pr}(Y \ge \epsilon^2)
\le
\frac{ \mathrm{E}[Y] } { \epsilon^2 }.
$$
Now 
$\mathrm{Pr}(Y \ge \epsilon^2) = \mathrm{Pr}(|Z - \overline Z| \ge \epsilon)$,
and
$\mathrm{E}[Y] = \sigma_Z^2$,
so we get \eqref{eq:chebyshev_ineq}.

This inequality shows that the deviation drops at least as fast as $1/\epsilon^2$.


\subsection{Chernoff's inequality}

Similarly for a positive random variable $Z$, and $r > 0$, the average of
the generating function $g_Z(r) = \mathrm{E}[e^{rZ}]$, if exists, satisfies
the Chernoff bound
\begin{equation}
\mathrm{Pr}(Z \ge z)
\le g_Z(r) \, \exp(-rz).
\label{eq:chernoff_ineq}
\end{equation}

Using Markov's inequality for $Y e^{rZ}$ we get
$$
\mathrm{Pr}(Y \ge e^{rz})
\le
\frac{ \mathrm{E}[Y] } { e^{rz} }.
$$
But $\mathrm{Pr}(Y \ge e^{rz}) = \mathrm{Pr}(Z \ge z)$,
and $\mathrm{E}[Y] = g_Z(r)$, so \eqref{eq:chernoff_ineq} is proven.

This shows that probability tail decays exponentially as $\exp(-rz)$.
This is what makes this bound useful.

\end{document}
