\documentclass{book}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    linkcolor={red!30!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\renewcommand{\thechapter}{\Roman{chapter}}
\makeatletter
  \renewcommand\l@chapter{\@dottedtocline{2}{1.5em}{2.5em}}
  \renewcommand\l@section{\@dottedtocline{2}{2.5em}{3.5em}}
\makeatother

\numberwithin{equation}{section}
\renewcommand{\theequation}{\textbf{\arabic{section}}.\arabic{equation}}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\theoremstyle{remark}
\newtheorem{rem}{Remark}

\begin{document}

% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{OliveGreen}\small [\textbf{Comment.} #1]}}

\newcommand{\hl}[1]{{\color{red} #1}}

\title{Notes on \emph{Stochastic Process in Physics and Chemistry}\cite{vankampen}}
\author{N. G. Van Kampen}
\date{3rd Edition (May 7, 2007)}

\maketitle

\tableofcontents


\chapter{Stochastic Variables}

\section{Definition}

\begin{defn}
  A continuous \emph{probability distribution}
  (\emph{probability density}) is a function
  that satisfies
  \begin{equation}
    P(x) \ge 0
    \label{eq:Px_nonneg}
  \end{equation}
  and
  \begin{equation}
    \int P(x) \, dx = 1.
    \label{eq:Px_normalize}
  \end{equation}
\end{defn}

For a mixture of discrete and continuous distribution,
\begin{equation}
  P(x) = \sum_n p_n \, \delta(x - x_n) + \tilde P(x).
  \label{eq:Px_mixture}
\end{equation}

The normalization is
$$
\sum_{n} p_n + \int \tilde P(x) \, dx = 1.
$$

\subsection*{Excursus}

\begin{defn}
  The \emph{cumulative distribution function}
  (\emph{probability distribution function})
  is defined as
  \begin{equation}
    \mathbb{P}(x) = \int_{-\infty}^{x+0} P(x') \, dx'.
  \end{equation}
\end{defn}

\paragraph{Axiomatic probability theory.}

References\cite{kolmogorov}.

\begin{enumerate}
\item The $x$-axis $\rightarrow$ a set $S$.
\item An interval $dx$ $\rightarrow$ a subset $A \subset S$.
\item Each subset $A$ is assigned
  a nonnegative number $\mathcal P(A)$, such that
  $$
  \mathcal P(S) = 1.
  $$
  For disjoint $A$ and $B$,
  $$
  \mathcal P(A + B) = \mathcal P(A) + \mathcal P(B).
  $$
  This is called the \emph{probability measure}.
\end{enumerate}

\subsection*{Exercises}

\paragraph{Rencontre problem (Matching problem).}

What's the probability that an arbitrary permutation
of $n$ objects leaves no object in its place?

Using the hint, we have
$$
n \, p(n) - (n-1) \, p(n-1) = p(n-2).
$$

Then
$$
n \, [p(n) - p(n-1)] = - [ p(n-1) - p(n-2)].
$$
and
$$
p(n) - p(n-1) = \frac{ (-1)^{n} }{n!}.
$$
So
$$
p(n) = \sum_{i = 0}^n \frac{ (-)^{k} } { k! } \rightarrow e^{-1}.
$$




\section{Averages}

Given a distribution $P(X)$, the average of $f(X)$ is computed by
$$
\langle f(X) \rangle
=
\int f(x) \, P(x) \, dx.
$$

A most commonly-used average is the $m$-th moment:
$$
\mu_m = \langle X^m \rangle.
$$
Here, $\mu_1$ is the mean, and
\begin{equation}
\sigma^2 =
\left\langle
  (X - \langle X \rangle )^2
\right\rangle
= \mu_2 - \mu_1^2,
\end{equation}
is the variance, and $\sigma$ is the standard deviation.

An interesting observation is that
not all probability distributions have a finite variance.
%
A famous example is the Lorentz distribution
\begin{equation}
P(x) = \frac{1}{\pi} \frac{\gamma}{ (x-a)^2 + \gamma^2 },
\qquad (-\infty < x < \infty).
\end{equation}

\subsection*{Exercises}

1. For the \emph{square distribution},
\begin{equation}
P(x) =
\begin{cases}
  0,          \qquad &\mathrm{for\;} |x| > a, \\
  (2a)^{-1},  \qquad &\mathrm{for\;} |x| < a. \\
\end{cases}
\end{equation}
We have $\mu_{2n+1} = 0$, and
$$
\mu^{2n}
=
\int_{-a}^a \frac{x^{2n}}{2a} \, dx
=
\frac{ a^{2n} }{ 2n + 1 }.
$$


2. For the \emph{Gauss distribution},
\begin{equation}
  P(x) = (2 \pi)^{-\frac 1 2} e^{-\frac{1}{2} x^2}.
\end{equation}
We have
$$
\begin{aligned}
  \mu_{2n+1} &= 0, \\
  \mu_{2n}  &= (2n-1)!!.
\end{aligned}
$$

3. Find a distribution whose moments $\mu_n$ exists up to but not beyond a prescribed $n$.

The answer is
$$
P(x) = \frac{ \alpha } { (x^2 + 1)^{1 + \frac{n}{2} } }.
$$
The normalization $\alpha$ is determined as follows.
$$
\begin{aligned}
1 &= \int_{-\infty}^\infty P(x) \, dx
  = \alpha \int_{-\pi/2}^{\pi/2} \frac{ \sec^2\theta } { \sec^{n+2}\theta } \, d\theta
  =\alpha \int_{-\pi/2}^{\pi/2} \cos^n\theta \, d\theta
  = B\left(\frac{n+1}{2}, \frac{1}{2}\right).
\end{aligned}
$$
So
$$
\alpha = \frac{ \Gamma(\frac n  2 + 1) } { \Gamma(\frac{n+1}{2}) \, \Gamma(\frac 1 2) }
=
\begin{cases}
  \frac{n!!}{(n-1)!! }\frac{1}{\pi}, & n \mathrm{\; is \; even} \\
  \frac{n!!}{(n-1)!! }\frac{1}{2}, & n \mathrm{\; is \; odd}
\end{cases}
$$

4. From
$$
f = \langle
|\lambda_0 + \lambda_1 \, X + \cdots + \lambda_n \, X^n|^2 \rangle \ge 0,
$$
we have
$$
\mathbf \Lambda^T
\left(
  \begin{array}{ccccc}
    1     & \mu_1 & \dots & \mu_n \\
    \mu_1 & \mu_2 & \dots & \mu_{n+1} \\
    \vdots& \vdots&       & \vdots \\
    \mu_n &\mu_{n+1}&\dots& \mu_{2n}
  \end{array}
\right)
\mathbf \Lambda
\ge 0.
$$
Since $\Lambda = (\lambda_0, \dots, \lambda_n)^T$ is arbitrary,
the matrix in the middle is nonnegative-definite,
meaning that every eigenvalue is nonnegative,
so is their product, or the determinant.
$$
\left|
  \begin{array}{ccccc}
    1     & \mu_1 & \dots & \mu_n \\
    \mu_1 & \mu_2 & \dots & \mu_{n+1} \\
    \vdots& \vdots&       & \vdots \\
    \mu_n &\mu_{n+1}&\dots& \mu_{2n}
  \end{array}
\right| \ge 0
$$


5. Eq. \eqref{eq:Px_nonneg} can be replaced with the condition
$$
\int f(x) P(x) \, dx \ge 0,
$$
for any continuous $f(x)$.


6. Show that for $n = 1, 2, 3, \dots$,
$$
P(x) = n \frac{e^{-x}}{x} I_n(x),
$$
has no average.

Notes.
$$
e^{ \frac{x}{2} \left( t + \frac{1}{t} \right) }
= \sum_{n = -\infty}^\infty I_n(x) \, t^n.
$$
And
$$
I_n(x) = \sum_{r = 0}^\infty \frac{1}{r! (r + n)!} \left(\frac x 2\right)^{n+2r}.
$$


\subsection*{Characteristic function}

\begin{defn}
The \emph{characteristic function} is defined as
\begin{equation}
G(k) = \langle e^{ikX} \rangle
=
\int e^{ikx} P(x) \, dx.
\end{equation}
\end{defn}

The characteristic function satisfies
\begin{equation}
  G(0) = 1, \qquad
  |G(k)| \le 1.
\end{equation}


The characteristic function is also the \emph{moment generating function}
because
\begin{equation}
  G(k) = \sum_{k = 0}^\infty \frac{(tk)^m } {m!} \mu_m.
  \label{eq:mugen}
\end{equation}
Similarly $\log G(k)$
\begin{equation}
  \log G(k) = \sum_{k = 0}^\infty \frac{(tk)^m } {m!} \kappa_m,
  \label{eq:kappagen}
\end{equation}
is the generating function of cumulants $\kappa_m$.
The first few are listed below.
\begin{equation}
  \begin{aligned}
    \kappa_1 &= \mu_1, \\
    \kappa_2 &= \mu_2 - \mu_1^2, \\
    \kappa_3 &= \mu_3 -3 \, \mu_2 \, \mu_1 + 2 \, \mu_1^3, \\
    \kappa_4 &= \mu_4 -4 \, \mu_3 \, \mu_1 - 3 \, \mu_2^2 + 12 \, \mu_2 \, \mu_1^2 - 6 \, \mu_1^4.
  \end{aligned}
\end{equation}


\section{Multivariate distributions}

\section{Addition of stochastic variables}

\section{Transformation of variables}

\section{The Gaussian distribution}

\section{The central limit theorem}




\chapter{Random Events}

\section{Definition}

\section{The Poisson distribution}

\section{Alternative description of random events}

\section{The inverse formula}

\section{The correlation functions}

\section{Waiting times}

\section{Factorial correlation functions}


\chapter{Stochastic Processes}

\section{Definition}

\section{Stochastic process in physics}

\section{Fourier transformation of stationary process}

\section{The hierarchy of distribution functions}

\paragraph{Stationary Process.}

A process is \emph{stationary} if all $P_n$ depend on the time difference alone.

$$
P_n(y_1, t_1 + \tau; y_2, t_2 + \tau; \cdots; y_n, t_n + \tau)
=
P_n(y_1, t_1; y_2, t_2; \cdots; y_n, t_n).
$$


\paragraph{Gaussian process.}

a process is \emph{Gaussian} if all $P_n$ are
(multivariate) Gaussian distributions.


\section{The vibrating string and random fields}

\section{Branching process}


\chapter{Markov Processes}


\section{Markov property}

\section{The Chapman-Kolmogorov equation}

Page 78.
A Markov process:
$$
P_{1|n-1}(y_n, t_n| y_1, t_1; \dots, y_{n-1}, t_{n-1})
=
P_{1|1}(y_n, t_n|y_{n-1},t_{n-1})
$$
where, $P_{1|1}$ is the transition probability.

Page 79.
Wiener process (Wiener-L\'evy process)
\begin{equation}
P_{1|1}(y_2, t_2|y_1, t_1)
=
\frac{1}{\sqrt{2\,\pi(t_2 - t_1)}}
\exp\left[
  -\frac{ (y_2 - y_1)^2 } { 2 \, (t_2 - t_1) }
\right].
\tag{2.4}
\end{equation}
%
\begin{equation}
P_1(y, t)
=
\frac{1}{\sqrt{2\,\pi\,t}}
\exp\left[
  -\frac{ y^2 } { 2 \, t }
\right].
\tag{2.5}
\end{equation}


Page 80.
Poisson process
\begin{equation}
P_{1|1}(n_2, t_2|n_1, t_1)
=
\frac{ (t_2 - t_1)^{n_2 - n_1} } { (n_2 - n_1)! }
e^{ -(t_2 - t_1) },
\tag{2.6}
\end{equation}
where
$P_1(n, 0) = \delta_{n,0}$.

Page 81. Ex. 5 Eq. (2.8)

$$
\frac{ \partial P } { \partial t }
=
D \frac{ \partial^2 P } { \partial y^2 }.
$$
Show (2.5) satisfies this by $D = \frac{1}{2}$.
Generally, try
$$
P = \frac{ 1 } { \sqrt{ 2 \, \pi \, a(t) } }
\exp\left(
  - \frac{ y^2 } { 2 \, a(t) }
\right).
$$
Then,
$$
\begin{aligned}
\frac{ \partial P } { \partial y }
&=
- \frac{ y } { a } P
\\
\frac{ \partial^2 P } { \partial y^2 }
&=
- \frac{1}{a} P
+ \frac{ y^2 } { a^2 } P
\\
\frac{ \partial P } { \partial t }
&=
-\frac{ a' } { 2 \, a } P
+ \frac{ y^2 } { 2 \, a^2 } a' P.
\end{aligned}
$$
So $a'(t) = 2 \, D(t)$,
and
$a(t) = 2 \int^t_0 D(s) \, ds$.


\section{3. Stationary Markov process}

Page 83.
Ornstein-Uhlenbeck (OU) process.
Phys. Rev. 36 823 (1930).
\begin{equation}
P_1(y_1)
=
\frac{1}{\sqrt{2 \, \pi} e^{-\frac 1 2 y_1^2 } }
\tag{3.10}
\end{equation}

\begin{equation}
  T_\tau(y_2 | y_1)
=
\frac{ 1 } { \sqrt{ 2 \, \pi \, (1 - e^{-2\tau}) }  }
\exp\left[
  -\frac{ (y_2 - y_1 e^{-\tau})^2 }
  { 2 ( 1 - e^{-2 \tau } ) }
\right]
\tag{3.11}
\end{equation}

Page 84.
Doob's theorem.
J. L. Doob Annals of Math. 43, 351 (1942).

If a process is stationary, Gaussian and Markovian,
essential OU is the only process with these properties.

Page 85.
Exercise 8. (3.11) satisfies
\begin{align}
\frac{ \partial T } { \partial \tau }
&=
\frac{ \partial } { \partial y_2 } \left( y_2 T \right)
+
\frac{ \partial^2 T } { \partial y_2^2 }
\quad \mathrm{forward}
\tag{3.20}
\\
\frac{ \partial T } { \partial \tau }
&=
-y_1 \frac{ \partial T } { \partial y_1 }
+
\frac{ \partial^2 T } { \partial y_1^2 }
\quad \mathrm{backward}
\tag{3.21}
\end{align}


Page 86.
Exercise 12.
Cauchy process.
\begin{equation}
  T_\tau(y_2|y_1)
  =
  \frac{1}{\pi}
  \frac{ \tau } { (y_2 - y_1)^2 + \tau^2 }.
\end{equation}
Prove it satisfies Chapman-Kolmogorov.
$$
\begin{aligned}
\int T_{\tau'}(y_3|y_2) T_\tau(y_2|y_1) \, dy_2
&=
\int \int \int \tilde T_\tau \, \tilde T_\tau' \,
e^{-i(k + k') y_2 } \frac{dk}{2\pi} \frac{dk'}{2\pi} dy_2
\\
&=
\int e^{ -i|k| (\tau +\tau') + ik (y_1 -y_3) } \frac{dk}{2\pi}
\\
&=
\frac{1}{\pi}
\frac{ \tau + \tau'}
{ (\tau + \tau')^2 + (y_3 - y_1)^2 } \\
&=
T_{\tau + \tau'}(y_3 | y_1) \\
\end{aligned}
$$



\section{The extraction of a subensemble}

\section{Markov chains}

\section{The decay process}


\chapter{The Master Equation}


\chapter{One-step Processes}


\chapter{Chemical reactions}


\chapter{The Fokker-Planck Equation}

\section{Introduction}

\begin{equation}
  \frac{ \partial P(y, t) }  { \partial t }
=
-\frac{\partial } {\partial y} A(y) P
+\frac{1}{2} \frac{ \partial^2 } { \partial y^2 } B(y) P.
\tag{1.1}
\end{equation}


\begin{equation}
\partial_t \langle y \rangle
= \langle A(y) \rangle.
\tag{1.7}
\end{equation}

$$
\partial_t \langle y \rangle
= A(\langle y \rangle).
$$

\section{Derivation of the Fokker-Planck equation}

\section{Brownian motion}

\section{The Rayleigh particle}

\section{Application to one-step process}

\section{The multivariate Fokker-Planck equation}

\begin{equation}
\frac{ \partial P } { \partial t }
=
-\sum_i \frac{ \partial } { \partial y_i } (A_i \, P)
+ \frac 1 2
\sum_{i, j} \frac{ \partial^2 } { \partial y_i \partial y_j } (B_{ij} P ).
\tag{6.1}
\end{equation}

To determine the parameters, we use
\begin{equation}
\begin{aligned}
\frac{ \langle \Delta y_i \rangle_y } { \Delta t }
&=
A_i(y), \\
\frac{ \langle \Delta y_i \, \Delta y_j \rangle_y } { \Delta t }
&=
B_{ij}(y).
\end{aligned}
\tag{6.3}
\end{equation}

The linear multivariate Fokker-Planck equation is
\begin{align}
\frac{  \partial P(y, t) } { \partial t }
=
-\sum_{i,j} A_{ij} \frac{ \partial } { \partial y_i } y_j P
+ \frac 1 2 \sum_{ij} B_{ij} \frac{ \partial^2 P } { \partial y_i \partial y_j },
\tag{6.4}
\end{align}

\section{Kramers' equation}

[
  This equation converts the position $X$ Fokkar-Planck equation
  to the position-velocity $X$-$V$ Fokkar-Planck equation.
]

\chapter{The Langevin Approach}

\section{Langevin treatment of Brownian motion}

\section{Applications}

\section{Relation of Fokker-Planck equation}

\section{The Langevin approach}

Page 230.

$$
\dot y = A(y) + C(y) L(t),
$$
where $L(t)$ is the Wiener process,
and $\langle L(t) L(t') \rangle = \Gamma \delta(t - t')$.


$$
\frac{ \partial P }{ \partial t }
=
-{ \partial } { \partial y } [ A(y) \, P ]
+
\frac{ \Gamma } { 2 } \frac{ \partial } { \partial y }
\left\{
  C(y)
  \frac{ \partial } { \partial y } [ C(y) \, P ]
\right\}.
$$

\section{Discussion of the It\^o-Stratonovich dilemma}

\section{Non-Gaussian white noise}

\section{Colored noise}

\chapter{The Expansion of the Master Equation}

\chapter{The Diffusion Type}

\chapter{First-passage Problems}

\chapter{Unstable Systems}

\chapter{Fluctuations in Continuous Systems}

\chapter{The Statistics of Jump Events}

\chapter{Stochastic Differential Equations}

\chapter{Stochastic Behavior of Quantum Systems}

\bibliographystyle{plain}
\bibliography{../simul}
\end{document}
