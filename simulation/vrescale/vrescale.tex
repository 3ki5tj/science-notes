\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[width=7.0in, height=10.0in]{geometry}



\begin{document}

\title{ Velocity rescaling thermostat }
\author{ \vspace{-10ex} }
\date{ \vspace{-10ex} }
\maketitle



The velocity rescaling thermostat is a popular modern thermostat
for molecular dynamics (MD) simulations.
%
Here, a ``thermostat'' is a piece of computer code
that helps the MD program to sample a canonical distribution.
%
Below we will explain how the thermostat works.



\section{Langevin equation}



We start by solving the following Langevin equation:
%
\begin{equation}
  \dot x = -\gamma x + \sqrt{ 2 \gamma \sigma^2 } \xi,
  \label{eq:ho_langevin}
\end{equation}
where,
$\dot x = dx/dt$,
$\gamma$ and $\sigma$ are two constants,
and $\xi$ is a unit Gaussian white noise:
\begin{align}
  \langle \xi(t) \xi(t') \rangle
=
  \delta(t - t').
\end{align}
This equation describes the motion of an over-damped harmonic oscillator
under a random noise\footnote{
%
%
%
The equation of motion of a harmonic oscillator
in a frictious environment
under a random noise is given by:
$M \dot v = -\mu v - K x + \sqrt{ 2 D } \, \xi$,
where, $v = \dot x$ is the velocity,
$M$ and $K$ are the mass and the spring constant, respectively,
and $\mu$ and $D$ are the mobility and diffusivity, respectively.
%
If the mass $M$ is large, we can drop the left-hand side,
and move the $\mu v$ term to the left.
%
This yields Eq. \eqref{eq:ho_langevin},
with $\gamma = K/\mu$,
and $\sqrt{2\gamma} \sigma = \sqrt{2 D}/\mu$,
%
or
$D = (\mu \sigma)^2 \gamma = K \sigma^2 \mu$.
%
Now Einstein's relation or the fluctuation-dissipation theorem
states that $\beta D = \mu$.
So
\begin{equation}
  \beta K \sigma^2 = 1.
  \label{eq:bKsig2}
\end{equation}
%
%
%
}.

Since Eq. \eqref{eq:ho_langevin} involves a random noise,
it is meaningless to discuss a particular trajectory.
%
Instead, we can imagine an \emph{ensemble} of systems
with the same initial condition,
and then study the averaged trajectory
and its variance at different times.
%
That is, we shall study the distribution
of $x$, $\rho(x, t)$ as a function of $t$.
%
At time $t = 0$, we can assume that the distribution
is limited at $x = x_0$:
%
\begin{equation}
  \rho(x, t = 0) = \delta(x - x_0).
  \label{eq:rho_t0}
\end{equation}



\section{Fokker-Planck equation}



We now give a useful theorem without giving a proof.
%
If $x$ follows the Langevin equation
\begin{equation}
  \dot x = -F(x) + \sqrt{ 2 B(x) } \, \xi,
  \label{eq:x_langevin}
\end{equation}
then the distribution $\rho(x, t)$ satisfies
\begin{equation}
\frac { \partial \rho(x, t) } { \partial t }
=
\frac { \partial } { \partial x }
\left\{
  F(x) \, \rho(x, t)
  +
  \frac { \partial  } { \partial x } \Bigl[ B(x) \, \rho(x, t) \Bigr]
\right\}.
\label{eq:x_fp}
\end{equation}

An immediate consequence of Eq. \eqref{eq:x_fp} is that
the stationary distribution, $\rho^*(x)$, satisfies
\begin{equation*}
  \frac { d } { d x } \bigl[ B(x) \, \rho^*(x) \bigr]
=
  - F(x) \, \rho^*(x)
\end{equation*}
or
\begin{equation}
  \frac { d } { d x } \log\bigl[ B(x) \, \rho^*(x) \bigr]
=
  - \frac{ F(x) }{ B(x) }.
\label{eq:rhost_fp}
\end{equation}

Equation \eqref{eq:rhost_fp} is useful.
%
If we know $F(x)$ and $B(x)$,
we can find out the stationary distribution $\rho^*(x)$
by solving Eq. \eqref{eq:rhost_fp}.
%
Conversely,
if we wish to design a dynamic equation for the random variable $x$
that satisfies some predefined distribution $\rho^*(x)$,
%
then, $F(x)$ and $B(x)$ are constrained by Eq. \eqref{eq:rhost_fp},
although we still have the freedom to choose a convenient $B(x)$.
%
The latter usage is employed in Sec. \ref{sec:tstat1} to derive
the velocity rescaling thermostat.



\section{\label{sec:rhost}
Stationary solution of the over-damped oscillator}



Before deriving the thermostat,
let us have some practice,
and apply Eq. \eqref{eq:rhost_fp} to Eq. \eqref{eq:ho_langevin}.
%
We identify that
$F(x) = \gamma \, x$,
and
$B(x) = \gamma \, \sigma^2$.
%
Thus, the stationary distribution, $\rho^*(x)$, satisfies
\[
  \frac { d } { d x } \log\bigl[ \gamma \sigma^2 \, \rho^*(x) \bigr]
=
  - \frac{ x }{ \sigma^2 }.
\]
This equation is readily solved,
\[
  \log \rho^*(x)
=
  - \frac{ x^2 }{ 2 \sigma^2 }
  + \mathrm{const.},
\]
which,
after the normalization $\int \rho^*(x) \, dx = 1$,
yields a Gaussian distribution:
\begin{align}
\rho^*(x)
=
\frac{ 1 } { \sqrt{ 2 \pi \sigma^2 } }
\exp\left(
  -\frac{ x^2 } { 2 \sigma^2 }
\right).
\label{eq:ho_rhost}
\end{align}

If $x$ is the coordinate of a harmonic oscillator,
then $\rho^*(x) \propto \exp( -\beta K x^2/2 )$,
which demands $\beta K \sigma^2 = 1$,
in agreement with Eq. \eqref{eq:bKsig2}.
%
We may also interpret $x$ as some momentum $p_i$,
%
then, $\beta \sigma^2 / M_i = 1$.



\section{General solution of the over-damped oscillator}



Now let us solve the time-dependent distribution, $\rho(x, t)$.
%
We shall assume the initial condition given by Eq. \eqref{eq:rho_t0},
%
and solve Eq. \eqref{eq:x_fp}.

Inspired by the stationary distribution, Eq. \eqref{eq:ho_rhost},
we shall use the trial solution
%
\begin{align}
\rho(x, t)
=
\frac{ 1 } { \sqrt{ 2 \pi b(t) } }
\exp \left\{
  -\frac{ [x - c(t)]^2 } { 2 b(t) }
\right\}.
\label{eq:ho_rhot_trial}
\end{align}
%
We expect that
i) at time $t = 0$,
$c(0) = x_0$, and $b(t) = 0$;
and
ii) at time $t \rightarrow \infty$,
$c(t) \rightarrow 0$, and $b(t) \rightarrow \sigma^2$.

Plugging Eq. \eqref{eq:ho_rhot_trial} into Eq. \eqref{eq:x_fp},
we get
\begin{multline}
  \frac{ 1 } { \sqrt{ 2 \pi b } }
  \exp \left[
    - \frac{ (x - c)^2 } { 2 b }
  \right]
  \left\{
    \left[
      \frac { (x - c)^2 } { b } - 1
    \right]
    \frac{ 1 } { 2 b }
    \frac{ d b } { d t }
    +
    \frac { x - c } { b }
    \frac { d c } { d t }
  \right\} \\
=
  \frac{ 1 } { \sqrt{ 2 \pi b } }
  \exp \left[
    - \frac{ (x - c)^2 } { 2 b }
  \right]
  \left\{
    \left[
      \frac{ (x - c)^2 } { b } - 1
    \right]
    \, \gamma \,
    \left(\frac{ \sigma^2 } b - 1\right)
    -
    \frac{ (x - c) } { b } \, \gamma \, c
  \right\}.
\end{multline}
%
Thus, for the trial solution to be valid,
we must have
\begin{align*}
\frac{ 1 } { 2 b }
\frac{ d b } { d t }
&=
\gamma \,
\left(\frac{ \sigma^2 } b - 1\right),
\\
\frac { d c } { d t }
&=
-\gamma \, c.
\end{align*}
Solving the above two equations, we get
\begin{align*}
b &= \sigma^2 \bigl[1 - \exp( - 2 \gamma t) \bigr],
\\
c &= c(0) \exp(-\gamma t) = x_0 \, \exp(-\gamma t).
\end{align*}
%
Thus, the solution is
%
\begin{align}
\rho(x, t)
=
\frac{ 1 } { \sqrt{ 2 \pi \sigma^2 (1 - e^{-2\gamma t}) } }
\exp \left[
  -\frac{ ( x - x_0 \, e^{-\gamma t} )^2 } { 2 \sigma^2 (1 - e^{-2\gamma t}) }
\right].
\label{eq:ho_rhot}
\end{align}
In other words,
the distribution at time $t$ is
a Gaussian centered at $c = x_0 \, e^{-\gamma t}$,
of variance $b = \sigma^2 (1 - e^{-2\gamma t})$.



\section{\label{sec:int0}Integrator}



For a short time $t \ll 1/\gamma$,
we have $c = x_0 (1 - \gamma t)$
and $b = 2 \sigma^2 \gamma t$.
That is
\[
  x(t)
\approx
  x_0 - \gamma \, x_0 \, t
  + \sqrt{ 2 \gamma \sigma^2 \, t } R,
\]
where $R$ is a unit-variance Gaussian random number.
%
This serves as the integrator for the Langevin equation.
%
Generally, the integrator for Eq. \eqref{eq:x_langevin}
is given by
\begin{equation}
  x(t)
\approx
  x_0 - F(x_0) \, t
  + \sqrt{ 2 B(x_0) \, t } \, R.
  \label{eq:langevin_int0}
\end{equation}



\section{\label{sec:tstat1}Velocity rescaling thermostat}



The velocity rescaling thermostat is essentially
a Langevin equation of the kinetic energy
$E_K = \sum_i p_i^2 / (2 M_i)$.
%
To derive it,
we need to first find out the stationary distribution $\rho^*(E_K)$
and then apply Eq. \eqref{eq:rhost_fp}.

The distribution of $\rho^*(E_K)$ can be computed as
an average in the canonical ensemble:
\begin{align}
\rho^*(E_K)
&=
\left\langle
  \delta\left( E_K - \sum\nolimits_i \tfrac{ p_i^2 }{ 2 M_i } \right)
\right\rangle
\notag \\
&=
\int \prod\limits_i d p_i d q_i \,
\delta\left( E_K - \sum\nolimits_i \tfrac{ p_i^2 }{ 2 M_i } \right)
\left.
\exp\left[
    - \beta \sum\nolimits_i \tfrac{ p_i^2 }{ 2 M_i }
    - \beta U(\{q_i\})
\right] \middle/Q \right.
\notag \\
&=
\beta^{N_f} E_K^{N_f/2 - 1} \exp(-\beta E_K) / \Gamma(N_f/2).
\label{eq:rhost_EK}
\end{align}
%
where $N_f$ is the number of degrees of freedom,
$Q$ is the partition function of the canonical ensemble at $\beta$,
and the factor $E_K^{N_f/2 - 1} d E_K$
comes from $\prod_{i = 1}^{N_f} d p_i$\footnote{
The gamma function $\Gamma(z) \equiv \int_0^\infty t^{z - 1} \exp(-t) \, dt$
  is equal to the usual factorial $(z-1)! = 1 \cdot 2 \cdots (z - 1)$ for an integral $z$.
If $z$ is an half of an odd integer like $3/2$,
  it is equal to $(2 z - 2)!! \sqrt\pi/2^{z - 1/2}$,
e.g., $\Gamma(1/2) = \sqrt\pi$,
$\Gamma(3/2) = \frac 1 2 \sqrt\pi$,
$\Gamma(5/2) = \frac { 3 \cdot 1 } { 2^2 } \sqrt\pi$,
$\Gamma(7/2) = \frac { 5 \cdot 3 \cdot 1 } { 2^3 } \sqrt\pi$,
\dots
}.
%
Thus
\[
\frac d {d E_K} \log \rho^*(E_K)
=
\frac{ N_f/2 - 1 } { E_K } - \beta.
\]


Now if we use Eq. \eqref{eq:rhost_fp},
with $x = E_K$ and $B(x) = (E_K/\beta) (2 \gamma)$,
then $F(E_K) = -\left(\frac { N_f } { 2 \beta } - E_K\right) \, (2 \gamma)$
(here $\gamma$ is some constants),
and we reach the Langevin equation
for the velocity-rescaling thermostat:
\begin{align}
\frac { d E_K } { d t }
=
  \left( \frac{ N_f }{ 2 \beta } - E_K \right)
  (2 \gamma)
+ \sqrt{ \frac{ 2 \, E_K } { \beta } ( 2 \gamma ) } \, \xi.
\label{eq:vrescale}
\end{align}




\section{Integrator for the velocity rescaling thermostat}



By Eq. \eqref{eq:langevin_int0},
the straightforward way of integrating Eq. \eqref{eq:vrescale} is
\begin{align}
E_K^\mathrm{new}
=
E_K^\mathrm{old}
+ \left( \frac{ N_f }{ 2 \beta } - \, E_K^\mathrm{old} \right)
  (2 \gamma \Delta t)
  + \sqrt{ \frac{ 2 E_K^\mathrm{old} } { \beta } (2 \gamma \Delta t) } \, R.
\label{eq:vrescale_int0}
\end{align}
%
where $R$ is a unit Gaussian random number.
After $E_K$ is updated,
we scale the velocity of all particles by a factor of
$\sqrt{ E_K^\mathrm{new} / E_K^\mathrm{old} }$.


This integrator, however,
only works for a small time step $\Delta t$.
%
In the Appendix of the original paper\cite{bdp2007},
the authors give an elegant exact integrator
for arbitrary time steps.
%
The trick is
to abandon \eqref{eq:vrescale},
and go back to the variables $p_i$'s.
%
If each momentum $p_i$ ($i = 1, \dots, N_f$) follows Eq. \eqref{eq:ho_langevin},
with $\sigma^2 = M_i/\beta$ (see Sec. \ref{sec:rhost}),
then the stationary distribution is
$\propto \exp[-\beta p_i^2/(2 M_i)]$,
and $E_K = \sum_i p_i^2 /(2 M_i)$
automatically satisfies Eq. \eqref{eq:rhost_EK}.

The advantage of working with the variables $p_i$
is that the exact time evolution
is given by Eq. \eqref{eq:ho_rhot}.
%
This yields the following crude algorithm:
%
\begin{enumerate}
  \item
    Find a set of \emph{virtual} momenta $\{ p_i(t = 0) \}$ such that
    \begin{equation}
      \sum_i p_i^2(0)/(2 M_i) = E_K^\mathrm{old}.
      \label{eq:sumEKold}
    \end{equation}

  \item
    Compute each $p_i(\Delta t)$ according to Eq. \eqref{eq:ho_rhot}.
    That is,
    $p_i(\Delta t) = p_i(0) e^{-\gamma \Delta t}
    + \sigma \sqrt{1 - e^{-2\gamma \Delta t}} R_i$,
    with $R_i$ being a unit Gaussian random number.
    Here, $\sigma^2 = M_i/\beta$.

  \item
    Compute
    $E_K^\mathrm{new}
    =
    \sum_i p_i^2(\Delta t)/(2 M_i)$.
\end{enumerate}

Note that the virtual momenta $\{p_i\}$
  can be different from the real ones,
  as long as they satisfy Eq. \eqref{eq:sumEKold}.
%
Thus, we can choose a set of $\{p_i(t = 0)\}$
  that is most convenient.
%
We set
$p_1(0) = \sqrt{2 M_1 E_K^\mathrm{old}}$,
and
$p_2(0) = \cdots = p_{N_f}(0) = 0$.
%
This yields\cite{bdp2007}
\begin{align}
  E_K^\mathrm{new}
&=
  p_1^2(\Delta t)/(2 M_1)
  + \sum_{i = 2}^{N_f} p_i^2(\Delta t) / (2 M_i)
= \frac{1}{2 M_1}
  \left(\sqrt{2 M_1 E_K^\mathrm{old}} + \sigma s R_1\right)^2
  + \sum_{i = 2}^{N_f} (\sigma s R_i)^2 / (2 M_i)
  \notag \\
&= E_K^\mathrm{old} \, c^2
  + \sqrt{ \frac {2 E_K^\mathrm{old}} {\beta} } \, c \, s \, R_1
  + \frac{ s^2 }{ 2 \beta }
  \left( R_1^2 + \sum_{i = 2}^{N_f} R_i^2 \right),
 \label{eq:vrescale_int}
\end{align}
%
where $c = e^{-\gamma \Delta t}$
and $s = \sqrt{1 - e^{-2\gamma \Delta t}}$.
%
In the limit of $\Delta t \ll 1$,
we have
\begin{align*}
  E_K^\mathrm{new}
= E_K^\mathrm{old}
+ \left(
    \frac { \sum_{i = 1}^{N_f} R_i^2 } { 2 \beta }
    - \, E_K^\mathrm{old}
  \right) \, (2 \gamma \Delta t)
  + \sqrt{ \frac {2 E_K^\mathrm{old} } {\beta} (2 \gamma \Delta t) }
    R_1.
\end{align*}
If we further replace $\sum_{i = 1}^{N_f} R_i^2$ by its mean $N_f$,
we recover Eq. \eqref{eq:vrescale_int0}.

In implementing Eq. \eqref{eq:vrescale_int},
we do not actually generate $N_f$ Gaussian random numbers, $R_i$'s.
%
For Eq. \eqref{eq:vrescale_int} needs only the sum $S \equiv \sum_{i = 2}^{N_f} R_i^2$,
which satisfies the $\chi^2$ distribution of $N_f - 1$ degrees
(a special gamma distribution).
%
Thus, we just need to call the routine that generates
a single random number that satisfies the $\chi^2$ or gamma distribution
for the sum.



\section{$\chi^2$ distribution}


Given $N$ unit-variant Gaussian random variables $x_i$,
the sum of the squares $z = \sum_{i = 1}^N x_i^2$
satisfies the $\chi^2$ distribution of $N$ degrees.

Now let us show that in the $N=2$ case,
the distribution of $z$ is exponential.
\begin{align*}
\rho(z)
&=
\int dx_1 \int dx_2
  \frac{1} {\pi}
  \exp(-x_1^2 - x_2^2)
  \,
  \delta(y - x_1^2 - x_2^2)
\\
&=
\int_0^\infty r dr \int_0^{2\pi} d\theta
  \frac{1} {\pi}
  \exp(-r^2)
  \,
  \delta(y - r^2)
= \exp(-y),
\end{align*}
where, we have changed to the polar coordinates
in the second step.

Generally, for an even number $N$,
$z$ is the sum of $N/2$ exponential random variables.
%
In the next section,
we will show that $z$
satisfies the gamma distribution
%
(and this result is true also for an odd $N$).



\section{Random number that satisfies the gamma distribution}



The distribution density of the gamma distribution is given by
\begin{equation}
  \rho_\nu(z) = z^{\nu - 1} \exp(-z) / \Gamma(\nu).
\end{equation}
The definition of $\Gamma(\nu)$
$\bigl[ \equiv \int_0^\infty z^{\nu - 1} \exp(-\nu) \, dz \bigr]$
ensures that $\rho_\nu(z)$ is properly normalized
$\int \rho_\nu(z) \, dz = 1$.

The special case of $\nu = 1$ is the exponential distribution:
\begin{equation}
  \rho_1(z) = \exp(-z).
\end{equation}
The cumulative distribution function (CDF)
$P_1(z) = \int_0^z \rho_1(y) \, dy$
is given by
$P_1(z) = 1 - \exp(-z)$.
Thus, a random number that satisfies the exponential distribution
is given by $-\log(1 - r)$,
where $r$ is a uniformly distributed random number between 0 and 1.

We can show that for an integral $\nu$,
a random number that satisfies the gamma distribution
can be constructed by adding up $\nu$ random numbers
drawn from the exponential distribution\footnote{
%
%
%
We prove this by induction.
The statement is obviously true for $\nu = 1$.
%
Suppose it remains true for $\nu = k$,
that is,
the sum $y$ of $k$ random numbers
from the exponential distribution
satisfies the gamma distribution
$\rho_k(y) = y^{k - 1} \exp(-y) / (k-1)!$.
%
Then, the sum $z$ of $k+1$ random numbers
can be treated as the sum $y$ of the first $k$ random numbers
and the last one $x$.
%
So
\begin{align*}
  \rho(z)
= \int_0^\infty \rho_k(y) \, dy \int_0^\infty \exp(-x) \, dx \, \delta(x + y - z)
= \int_0^z y^{k - 1} \exp(-z) \, dy / (k-1)! = z^k \exp(-z) / k!.
\end{align*}
%
Thus, $z$ satisfies the gamma distribution $\rho_{k+1}$.
This completes the induction.
%
%
%
}.
This method can be used to generate a random number
that satisfies the gamma distribution for a small $\nu$.
For a larger $\nu$, the ratio method is preferred
(see Numerical Recipes).



\begin{thebibliography}{100}

\bibitem{bdp2007}
  Giovanni Bussi, Davide Donadio and Michele Parrinello,
  ``Canonical sampling through velocity rescaling,''
  J. Chem. Phys., Vol. 126, 014101,
  2007;
  also arXiv:0803.4060.

\end{thebibliography}



\end{document}
