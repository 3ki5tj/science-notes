\documentclass[12pt]{article}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}



\begin{document}



\title{A note on the left-right estimator}
\author{ \vspace{-10ex} }
\date{ \vspace{-10ex} }
\maketitle



\paragraph{Basic identity.}



We start with the following identity
\begin{align}
  E(x)
&=
  \int_{x_{\min}}^x \phi'(y) \, E(y) \, dy
  +
  \int_x^{x_{\max}} \phi'(y) \, E(y) \, dy
\notag
\\
&
  +
  \int_{x_{\min}}^x \phi(y) \, E'(y) \, dy
  +
  \int_x^{x_{\max}} \phi(y) \, E'(y) \, dy,
\label{eq:lr0}
\end{align}
if the following conditions hold
%
\begin{align}
  \phi(x_{\min}) = \phi(x_{\max}) = 0,
  \label{eq:phi_boundary}
\\
  \phi(x^{-}) - \phi(x^{+}) = 1.
  \label{eq:phi_normalization}
\end{align}


\paragraph{Proof.}



This identity is derived from
%
\begin{align*}
  \phi(y) \, E(y) \Big|_{x_{\min}}^{x_{\max}}
&=
  \int [\phi'(y) \, E(y) + \phi(y) \, E'(y)] \, dy.
\end{align*}
%
Then, by Eq. \eqref{eq:phi_boundary},
the left-hand side vanishes.
%
The integral on the right-hand side
can be broken up to three pieces,
and by Eq. \eqref{eq:phi_normalization},
we have
\[
0=-E(x)
+
  \int_{x_{\min}}^{x} [\phi'(y) \, E(y) + \phi(y) \, E'(y)] \, dy
+
  \int_{x}^{x_{\max}} [\phi'(y) \, E(y) + \phi(y) \, E'(y)] \, dy,
\]
which is Eq. \eqref{eq:lr0}.



\paragraph{Left-right estimator.}



In the left-right estimator,
we demand the sum of the last two terms
on the right-hand side of Eq. \eqref{eq:lr0} to vanish,
\begin{align}
  \int_{x_{\min}}^x \phi(y) \, E'(y) \, dy
+
  \int_x^{x_{\max}} \phi(y) \, E'(y) \, dy
= 0.
\label{eq:lr1_constraint}
\end{align}
%
Then we have a simplified identity
\begin{align}
  E(x)
&=
  \int_{x_{\min}}^x \phi'(y) \, E(y) \, dy
  +
  \int_x^{x_{\max}} \phi'(y) \, E(y) \, dy.
\label{eq:lr1}
\end{align}



\paragraph{Choice of the function $\phi(x)$.}



We now consider the following $\phi(x)$
whose derivative satisfies
\begin{align}
  \phi'(y)
=
  \begin{cases}
    a \, w(y) &   \mbox{for $y < x$,} \\
    b \, w(y) &   \mbox{for $y > x$,}
  \end{cases}
\end{align}
where $w(x)$ is the ensemble weight.
%
Thus,
\begin{align}
  \phi(y)
=
  \begin{cases}
    a \, W_-(y) = a \int_{x_-}^y w(z) \, dz   &   \mbox{for $y < x$,} \\
    b \, W_+(y) = -b \int_y^{x_+} w(z) \, dz  &   \mbox{for $y > x$.}
  \end{cases}
  \label{eq:phi_W}
\end{align}
%
Then, Eq. \eqref{eq:phi_normalization} means
\begin{align}
  a \int_{x_-}^x w(y) \, dy
+
  b \int_{x}^{x_+} w(y) \, dy
=
  1.
  \label{eq:phi_normalization1}
\end{align}



\paragraph{Connection to the histogram.}



The observed distribution, $\hat w(x)$,
should match $w(x)$,
\begin{align}
  \hat w(x) = w(x).
  \label{eq:h_w}
\end{align}
%
This means that
we can substitute $\hat w(x)$ for $w(x)$ in
Eqs. \eqref{eq:phi_normalization}-\eqref{eq:lr1}.
%
That is,
$a$ and $b$ satisfy
\begin{align}
  &a \, \int_{x_-}^x \hat w(y) \, dy
  &+\phantom{=}
  &b \, \int_x^{x_+} \hat w(y) \, dy
  &=\phantom{=}
  &1
  \label{eq:h_normalization}
\\
  &a \int_{x_{-}}^x W_-(y) \, E'(y) \, dy
&+\phantom{=}
  &b \int_x^{x_{+}} W_+(y) \, E'(y) \, dy
  &=\phantom{=}
  &0.
\label{eq:lrw_constraint}
\end{align}
%
and,
\begin{align}
  E(x)
&=
  a \int_{x_{-}}^x \hat w(y) \, E(y) \, dy
+
  b \int_x^{x_{+}} \hat w(y) \, E(y) \, dy.
\label{eq:lrw}
\end{align}



Now
\[
  \int_{x - \Delta x/2}^{x + \Delta x/2} \hat w(y) \, dy
\]
can be estimated by the total number of data points
that fall in the bin $(x - \Delta x/2, x + \Delta x/2)$,
or the normalized histogram height $h(x)$ at $x$,
and
\[
  \int_{x - \Delta x/2}^{x + \Delta x/2} \hat w(y) \, E(y) \, dy
\]
can be estimated as the total energy of the data points.



\end{document}
