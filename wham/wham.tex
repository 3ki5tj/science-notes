%\documentclass[reprint,aip,jcp,superscriptaddress]{revtex4-1}
\documentclass[aip,jcp,preprint,superscriptaddress]{revtex4-1}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}

\hypersetup{
    colorlinks,
    linkcolor={red!30!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}


\begin{document}



\newcommand{\vct}[1]{\mathbf{#1}}
\newcommand{\vx}{\vct{x}}
\newcommand{\vy}{\vct{y}}
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\Ham}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\com}{\mathrm{com}}

\newcommand{\llbra}{[\![}
\newcommand{\llket}{]\!]}

% annotation macros
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{OliveGreen}\small [\textbf{Comment.} #1]}}





\title{Introduction to the weighted histogram analysis method (WHAM)}
%\author{ \vspace{-10ex} }
%\date{ \vspace{-10ex} }


\begin{abstract}
  We review the weighted histogram analysis method (WHAM)
  and some related free energy methods.
  %
  We shall give three derivations of WHAM,
  from the composite-ensemble viewpoint,
  the maximum-likelihood method,
  and the classical variance analysis.
  %
  The iteration-free version, ST-WHAM,
  multiple Bennett's acceptance ratio (MBAR) method
  are also reviewed.

  If the reader is interested in a short
  derivation of WHAM, sections
  \ref{sec:basics} to \ref{sec:WHAM_selfconsistent} (about four pages)
  would suffice.
\end{abstract}

\maketitle

\tableofcontents



\section{\label{sec:intro}
What is WHAM and what does it do?}



Our main purpose is
to give a detailed discussion on
an elegant free energy calculation method called
the weighted histogram analysis method (WHAM)\cite{
ferrenberg1988, *ferrenberg1989, kumar1992,
newman, frenkel}.
%
The method is widely used because
it gives the most accurate and precise result
among a large class of similar methods.
%
Nonetheless,
the common derivation in literature
is somewhat complicated.
%
Here, we shall see that
WHAM can also be derived
using some very rudimentary arguments.



Although WHAM is a general method,
we shall discuss it in the context of the following case\cite{
ferrenberg1988, *ferrenberg1989, newman}.
%
Suppose we have performed Monte Carlo (MC) simulations of a given system,
at several temperatures $\beta_1$, $\beta_2$, \dots, $\beta_K$
%
[here $\beta = 1/(k_B T)$ denotes the inverse temperature].
%
We can then readily compute the average energy, heat capacity, etc.,
at these temperatures.


Now the first question is that
can we also \emph{rigorously} compute these quantities at a temperature,
say $\beta = (\beta_1 + \beta_2)/2$, which is not simulated?
%
A cheap answer such as a linear interpolation, for example,
would be biased and inefficient.
%
But WHAM gives a better solution.
%
The quantities computed from WHAM can usually be
much more accurate (unbiased)
and precise (with minimal errors).



How is this possible?
%
Well, the data obtained from MC simulations
are not arbitrary, but subject to
some underlying constraints from statistical mechanics.
%
Particular, as we shall see,
the energy distributions from the different temperatures
share a common factor called the density of states,
and they differ only by some external weighting factor.
%
This interrelation between distributions
makes \emph{sharing} data among simulations possible.
%
This sharing in turn leads to WHAM,
a method that maximizes the statistical efficiency
of using all available data.
%
Thus, even if we only care about the quantities
at a particular temperature,
the data from \emph{all} simulated temperatures
can be combined in an unbiased way
to improve the estimates of the quantities.



Besides, WHAM will provide accurate
estimates for the partition function,
or, equivalently, the free energy,
which is usually something valuable
but hard-to-get.
%
This is why WHAM is usually introduced
as a free energy method.




\section{\label{sec:basics}
Basics}



\subsection{Canonical ensemble}



We first need to review some basic statistical mechanics.
%
For simplicity,
we assume a quantum system with discrete energy levels.
%
The density of states, $g(E)$, is defined as the number of states
sharing the same energy $E$.
%
This $g(E)$
is also the partition function of
the microcanonical ensemble.
%
In the canonical ensemble,
the partition function
at a certain temperature $\beta$
is given by
%
\begin{equation}
  Z(\beta) = \sum_E g(E) \, \exp(-\beta \, E).
  \label{eq:Z_def}
\end{equation}



The energy distribution is given by
%
\begin{equation}
  p_\beta(E) = g(E) \, \frac{ \exp(-\beta \, E) } { Z(\beta) }
  = g(E) \, w_\beta(E),
  \label{eq:pE_def}
\end{equation}
%
where we have introduced
a normalized weight\footnote{
This weight is useful
because in many formulae below,
$Z(\beta)$ always goes with $\exp(-\beta E)$}
\begin{equation}
  w_\beta(E) = \frac{ \exp(-\beta E) }{ Z(\beta) }.
\label{eq:wE_def}
\end{equation}
%
It is clear from Eq. \eqref{eq:Z_def}
that we have the normalization:
\begin{equation}
  1 = \sum_E p_\beta(E) = \sum_E g(E) \, w_\beta(E).
  \label{eq:pE_normalization}
\end{equation}
%



With the energy distribution,
we can compute the average of any quantity,
$A(E)$, that can be expressed as a function of $E$:
%
\begin{equation}
  \langle A(E) \rangle_\beta
=
  \sum_E A(E) \, p_\beta(E).
  \label{eq:pE_average}
\end{equation}
%
For example,
if we want the average energy,
we simply set $A(E) = E$.



From a simulation at temperature $\beta$,
we can collect an energy histogram, $n_\beta(E)$,
whose entries sum to the sample size.
%
\begin{equation}
  N_\beta = \sum_E n_\beta(E).
  \label{eq:nE_sum}
\end{equation}
%
The normalized histogram
should match the theoretical distribution
\begin{align}
  \frac{ \langle n_\beta(E) \rangle } { N_\beta }
=
  p_\beta(E)
&=
  g(E) \, w_\beta(E)
\label{eq:nEw}
\\
&=
  g(E) \frac{ \exp(-\beta \, E) } { Z(\beta) }.
\label{eq:nE}
\end{align}




\subsection{General ensemble}



A similarly set of formulae
can be derived for a general ensemble.
%
In the general case,
we postulate a nonnegative weight $w_q(E)$,
which generalizes the factor $w_\beta(E) = \exp(-\beta E)/Z(\beta)$ above.
%
The energy distribution is given by
%
\begin{equation}
  p_q(E) = g(E) \, w_q(E).
  \label{eq:pqE_def}
\end{equation}
%
The distribution
has to be normalized:
\begin{equation}
  1 = \sum_E p_q(E)
  = \sum_E g(E) \, w_q(E).
  \label{eq:pqE_normalization}
\end{equation}


For a simulation done in this ensemble,
we have, similar to Eq. \eqref{eq:nEw},
\begin{equation}
  \frac{ \langle n_q(E) \rangle_q } { N_q }
= p_q(E)
= g(E) \, w_q(E).
  \label{eq:nqE}
\end{equation}





\section{\label{sec:WHAM_compens}
Deriving WHAM from the composite ensemble}



\subsection{Estimating the density of states}


We have now all the tools needed to derive WHAM.
%
An important observation from
Eqs. \eqref{eq:pE_def} and \eqref{eq:pqE_def}
is that
the energy distributions at different ensembles
differ only by the ensemble weight,
which is $w_\beta(E) = \exp(-\beta \, E) / Z(\beta)$
for the canonical ensemble,
or $w_q(E)$ in the general case.
%
The common part, the density of states,
$g(E)$, is the key for WHAM.
%
If we can accurately estimate $g(E)$,
then for any temperature, $\beta$,
we can compute $Z(\beta)$ from Eq. \eqref{eq:Z_def},
and Eq. \eqref{eq:pE_def} gives the distribution,
hence, any average quantities.
%
Thus,
the central task of WHAM
is to estimate $g(E)$ accurately.



The simplest way
of estimating $g(E)$ is through the histogram equation,
Eq. \eqref{eq:nE}:
%
\begin{align}
\hat g(E)
=
\frac{ n_\beta(E) }
{ N_\beta \, w_\beta(E) }
=
\frac{ n_\beta(E) }
{ N_\beta \, \exp(-\beta \, E) / Z(\beta) }
\label{eq:gE_onehistogram}
\end{align}
for some $\beta = \beta_k$.
%
This is obviously not the most efficient way,
because we are only using the data
from a single simulation.
%
To improve this,
we naturally would like to include data
from all simulations.
%
Below is how to do it (the WHAM way).





\subsection{Composite ensemble}



Imagine a composite ensemble,
which is a simple superposition of
the $K$ canonical ensembles.
%
By its very nature,
this ensemble includes all simulation data
and should fit our need.
%
The histogram of this ensemble
is obviously the overall histogram
%
\begin{equation}
  n_\com(E)
=
  \sum_{k = 1}^K n_k(E),
  \label{eq:ncom_def}
\end{equation}
%
where $n_k(E) \equiv n_{\beta_k}(E)$.
%
If we sum over $E$,
we should get the total sample size
%
\begin{equation}
\sum_E n_\com(E)
=
\sum_{k = 1}^K N_k
=
N_\com.
\label{eq:ncom_sum}
\end{equation}
%
where $N_k \equiv N_{\beta_k}$.



Let us find out the ensemble weight, $w_\com(E)$,
of the composite ensemble.
%
Since the composite ensemble is a simple superposition,
the weight should be a linear combination
of the weights of the member ensembles,
with the sample sizes serving as the relative weights:
%
\begin{equation}
w_\com(E)
=
\sum_{k = 1}^K
\frac{ N_k } { N_\com }
w_{\beta_k}(E)
=
\sum_{k = 1}^K
\frac{ N_k } { N_\com }
\frac{ \exp( -\beta_k \, E ) } { Z(\beta_k) }.
\end{equation}
%
It is easy to check that this weight is properly normalized
according to Eq. \eqref{eq:pqE_normalization}:
%
\begin{align*}
\sum_E g(E) \, w_\com(E)
&=
\sum_{k = 1}^K
\frac{ N_k } { N_\com }
\left(
  \sum_E g(E) \, w_{\beta_k}(E)
\right)
\\
&=
%\sum_{k = 1}^K
%\frac{ N_k } { N_\com }
%\frac{ Z(\beta_k) } { Z(\beta_k) }
%=
\sum_{k = 1}^K
\frac{ N_k } { N_\com }
=1.
\end{align*}




Now let us solve $g(E)$
for the composite ensemble,
from Eq. \eqref{eq:nqE}
with $q \rightarrow \com$:
%
\begin{align}
\hat g(E)
&=
\frac{ n_\com(E) }
{ N_\com \, w_\com(E) }
\notag \\
&=
\frac{ \sum_{k = 1}^K n_k(E) }
{ \sum_{k = 1}^K N_k \, \exp( -\beta_k \, E ) / Z(\beta_k) }.
\label{eq:gE_WHAM}
\end{align}
%
This can be also reached from an analogy
of Eq. \eqref{eq:gE_onehistogram}.
%
Since this formula uses all available data,
it should be the most precise estimate of $g(E)$.




\subsection{\label{sec:WHAM_selfconsistent}
Self-consistent solution}



This is it!
%
Equation \eqref{eq:gE_WHAM} is the key equation of WHAM.
%
But, wait, we don't know $Z(\beta_k)$ yet,
how can we use them in Eq. \eqref{eq:gE_WHAM}?
%
Well, this equation is meant to be solved
\emph{self-consistently}.
%
The simplest way for this is a direct iteration.
%
Initially,
we can guess a set of arbitrary
$Z(\beta_k)$,
say,
\[
  Z^{(0)}(\beta_1) = \dots = Z^{(0)}(\beta_K) = 1.
\]
%
Plugging these numbers to Eq. \eqref{eq:gE_WHAM}
yields a $g^{(1)}(E)$.
%
Then, we use this $g^{(1)}(E)$ in Eq. \eqref{eq:Z_def}
to compute a new set of
$Z^{(1)}(\beta_1), \dots, Z^{(1)}(\beta_K)$.
%
This time, the numbers should be better
than those from the last time.
%
Keep going back and forth
between Eqs. \eqref{eq:gE_WHAM} and \eqref{eq:Z_def}
will ultimately yield a set of converged
$Z^{(\infty)}(\beta_k)$,
and the corresponding density of states
$g^{(\infty)}(E)$,
which are the converged results of the iteration.
%
They represent the solution of the WHAM equation.



Note that the free energies and the density of states
are only determined up to a multiplication constant.
%
That is,
if we multiple $g(E)$ and $Z(\beta_k)$
by the same constant,
Eqs. \eqref{eq:Z_def} and \eqref{eq:gE_WHAM}
are unaffected.
%
Thus, we need to
fix a reference value for an anchor point,
e.g.,
$g(E_{\min}) = 1$,
or
$Z(\beta_1) = 1$.
%
Other values are then scaled accordingly.



\subsection{\label{sec:MBAR}
Histogram-free form: multiple Bennett's acceptance ratio (MBAR) method}



We now describe a generalization of WHAM
that avoids the histogram dependence.
%
Using Eq. \eqref{eq:gE_WHAM} in Eq. \eqref{eq:Z_def},
we get
\begin{align}
  Z_i
  =
  \sum_E
  \frac{ n_\com (E) \, q_i(E) }
       { N_\com \, w_\com(E) },
  \label{eq:Z_WHAM_sumE}
\end{align}
%
where,
\begin{align}
  q_i(E) &\equiv \exp(-\beta_i \, E), \\
  w_\com(E) &= \sum_{k = 1}^K
  \frac{ N_k } { N_\com }
  \frac{ q_k(E) } { Z_k }.
  \label{eq:wcom_def}
\end{align}



Now observe that
$n_\com(E)$
is the sum over configurations $\vx$ in the trajectories
with the same $E$.
%
In other words,
\begin{align}
  n_\com(E)
  =
  \sum_{\vx} \delta_{E, E(\vx)},
  \label{eq:nE_sumx}
\end{align}
%
where the sum $\sum_{\vx}$
is carried over all configurations, $\vx$,
in the trajectories (no matter the temperature),
and $E(\vx)$ means the energy of configuration $\vx$.
%
It follows that any sum of $n_\com(E)$ over $E$
is equivalent to a sum over configurations, $\vx$,
\begin{align*}
  \sum_E n_\com(E) \, A(E)
  &=
  \sum_E \sum_{\vx} \delta_{E, E(\vx)} \, A(E) \\
  &=
  \sum_{\vx} A(E(\vx)).
\end{align*}
%
If we use the above result for Eq. \eqref{eq:Z_WHAM_sumE}
with $A(E) = q_i(E)/[N_\com \, w_\com(E)]$,
then
%
\begin{align}
  Z_i
  &=
  \sum_{\vx}
  \frac{ q_i[ E(\vx) ] }
       { N_\com w_\com[ E(\vx) ] }
        \notag \\
  &=
  \sum_{\vx}
  \frac{ q_i[ E(\vx) ] }
  { \sum_k N_k \, q_k[ E(\vx) ] / Z_k }.
  \label{eq:Z_WHAM_sumx}
\end{align}
%
This form of WHAM is free of histograms,
so we shall refer to it as the histogram-free form.



Equation \eqref{eq:Z_WHAM_sumx}
is also the central result of
a generalization of WHAM called
the multiple Bennett's acceptance ratio (MBAR) method\cite{shirts2008}.
%
This name is due to that the method
was originally derived as a generalization of
Bennett's acceptance ratio (BAR) method\cite{bennett1976}.
%
It is a generalization
instead of a variant, because it permits
the weight $q_i$ to depend on the configuration $\vx$
in an arbitrary fashion.
%
That is, $q_i$ does not have to depend on $\vx$
through the energy $E(\vx)$ according to
$\exp[-\beta_i \, E(\vx)]$,
but instead,
we can have a distinct functional form
for each simulation condition $i$, that is
\[
  q_i(\vx) = \exp[-\beta_i \, E_i(\vx)].
\]
Note the subscript $i$ to $E(\vx)$,
meaning a different function for each $i$.
%
For example,
in simulation of a fluid,
we may wish to change the radii of interaction
in each simulation condition.
%
Then, the above MBAR form would still be valid,
whereas the original WHAM form is not.
%
To emphasize this freedom,
we rewrite Eq. \eqref{eq:Z_WHAM_sumx} as
\begin{equation}
  Z_i
  =
  \sum_{\vx}
  \frac{ q_i(\vx) }
  { \sum_{k = 1}^K N_k \, q_k(\vx) / Z_k }.
  \label{eq:Z_MBAR}
\end{equation}



\subsection{Derivation of the histogram-free form}



We now give a direct derivation
of the generalized form, Eq. \eqref{eq:Z_MBAR}.
%
We shall still adopt the composite ensemble view.
%
The key of the new derivation is to assume that
each configuration, $\vx$, is free to choose
a simulation condition $i$ (e.g., the simulation temperature $\beta_i$)
according to the ensemble weight.
%
Then, the probability of configuration, $\vx$,
assuming condition $i$ in the composite ensemble
is given by
%
\begin{align}
  p(i|\vx)
  &=
  \frac{ \dfrac{N_i}{N_\com} \dfrac{q_i(\vx)}{Z_i} }
  { w_\com(\vx) }
  %\notag \\
  =
  \frac{ \dfrac{N_i}{N_\com} \dfrac{q_i(\vx)}{Z_i} }
  { \sum_{k = 1}^K \dfrac{N_k}{N_\com} \dfrac{q_k(\vx)}{Z_k} }
  \notag \\
  &=
  \frac{ N_i \, q_i(\vx) / Z_i }
  { \sum_{k = 1}^K N_k \, q_k(\vx) / Z_k }.
  \label{eq:pix_MBAR}
\end{align}


If we sum Eq. \eqref{eq:pix_MBAR}
over all trajectory frames $\vx$
(no matter the simulation conditions),
we get the expected value of the population
in condition $i$:
%
\begin{align}
  \hat N_i
  &=
  \sum_{\vx} p(i|\vx)
  \notag \\
  &=
  \sum_{\vx}
  \frac{ N_i \, q_i(\vx) / Z_i }
  { \sum_{k = 1}^K N_k \, q_k(\vx) / Z_k }.
  \label{eq:Ni_WHAM}
\end{align}
%
If we demand the expected value $\hat N_i$
to be equal to observed one $N_i$,
we then recover Eq. \eqref{eq:Z_MBAR}
after multiplying both sides by $Z_i/N_i$.
%
This completes the derivation.






\section{\label{sec:WHAM_maxlikelihood}
Deriving WHAM from the maximum-likelihood method}



In this and the next sections,
we shall return to WHAM
and give two more popular derivations.
%
Although they are somewhat less straightforward,
it is good to know them,
because most people do not derive WHAM
using the composite ensemble approach.



Another way of reaching Eq. \eqref{eq:gE_WHAM}
is through a probability argument\cite{
bartels1997, *gallicchio2005, *habeck2007, *habeck2012, zhu2012}.
%
While it looks a bit formal,
the essence are pretty much the same.



The aim, as we recall, is to guess the most probable
density of states $g(E)$ from the observed trajectories.
%
The maximum-likelihood method is a general method of
estimating parameters of distributions,
and we shall use it to estimate $g(E)$.



\subsection{Functional differentiation}



We shall first make clear
what we mean by ``guessing the most probable $g(E)$''.
%
Since $g(E)$ is a function,
we actually mean guessing a set of \emph{parameters}
that characterize the function $g(E)$.
%



Intuitively,
we can imagine the function $g(E)$
as a curve determined by the values
$g_1 \equiv g(E_1)$,
$g_2 \equiv g(E_2)$,
\dots,
at a set of grid points
$E_1$, $E_2$, \dots,
which are the energy levels in our case.
%
Thus,
$g_1$, $g_2$, \dots
are the parameters that characterize the function $g(E)$,
and we will find the most probable values
of these numbers.



Mathematically,
we shall use the language called functional differentiation
which is designed for this type of task.
%
However, the grid points $E_1$, $E_2$, \dots,
are implicit in the language,
and we have to mentally picture them when needed.



A functional $F[g(E)]$ is something (called a mapping)
that takes a function as the input,
and outputs a number.
%
It is equivalent to a multi-variable function
$F(g_1, g_2, \dots)$.



A functional differentiation is
equivalent to a partial differentiation:
%
\begin{equation}
\frac{ \delta F[g(E)]}
     { \delta g(E_j) }
\rightarrow
\frac{ \partial F(g_1, g_2, \dots) }
     { \partial g_j }
\end{equation}
%
Below are some examples.
%
First,
%
\begin{equation}
\frac{ \delta g(E_i) }
     { \delta g(E_j) }
=
\frac{ \partial g_i }
     { \partial g_j }
=
\delta_{i, j},
\end{equation}
%
where $\delta_{a, b}$
is a two-variable function,
which yields either $1$ if $a = b$,
or $0$ otherwise.\footnote{
%
Here, we assume the energy is discrete.
In the continuous case,
$\delta_{a, b}$
should be replaced by the Dirac delta function
$\delta(a - b)$.
%
}
%
It is readily verified that
$\delta_{i, j} = \delta_{E_i, E_j}$.
%
Thus,
without writing the grid points explicitly, we have
\begin{equation}
\frac{ \delta g(E') }
     { \delta g(E) }
=
\delta_{E', E}.
\label{eq:funcdiff_g}
\end{equation}



Using this rule for the partition function $Z(\beta)$,
we have
\begin{align}
\frac{ \delta Z(\beta) }
     { \delta g(E) }
&=
\sum_{E'}
\frac{ \delta g(E') }
     { \delta g(E) }
\exp(-\beta \, E')
\notag \\
&=
\sum_{E'} \delta_{E', E} \, \exp(-\beta \, E')
\notag \\
&= \exp(-\beta \, E).
\label{eq:funcdiff_Z}
\end{align}




Other rules in the usual calculus
can be adopted here almost unchanged.
%
For example,
\begin{equation}
\frac{ \delta \log F[g(E)] }
     { \delta g(E) }
=
\frac{ 1 } { F[g(E)] }
\frac{ \delta F[g(E)] }
     { \delta g(E) }.
\label{eq:funcdiff_logF}
\end{equation}




\subsection{Conditional probability}



The first step is write down the conditional probability,
the probability of observing the trajectory,
for a given $g(E)$.
%
Basically,
it would be a product of
probabilities given by Eq. \eqref{eq:pE_def},
one for each trajectory frame.
%
\begin{align}
P[\mathrm{Traj.}|g(E)]
=
\prod_{k = 1}^K
\prod_{j = 1}^{N_k}
g(E^{(k)}_j) \, \frac{ \exp(-\beta \, E^{(k)}_j) } { Z(\beta_k) },
\label{eq:Pg_cond}
\end{align}
%
where
$E^{(k)}_j$
means the $j$th frame of simulation $k$.
%
The first product $\prod_{k=1}^K$
runs over simulations.
%
The second product $\prod_{j=1}^{N_k}$
runs over trajectory frames in the $k$th simulation.




\subsection{Likelihood function}



Viewed as a \emph{functional} of $g(E)$,
$P[\mathrm{Traj.}|g(E)]$
gives the likelihood of function $g(E)$,
given the observed trajectory.
%
Our job below is to figure out the function $g(E)$
that maximizes the likelihood function.
%
For mathematical convenience,
we shall work on the logarithm,
or the log-likelihood function,
\[
  \LL[g(E)]
  \equiv
  \log P[\mathrm{Traj.}|g(E)].
\]
%
We shall then use functional differentiation
to do the maximization.
%





Let us write down $\LL[g(E)]$ first,
%
\begin{align}
\LL[g(E)]
&=
\log P[\mathrm{Traj.} | g(E)]
\notag \\
&=
\sum_{k = 1}^K
\sum_{j = 1}^{N_k}
  \left[
  \log g( E^{(k)}_j )
  -\beta \, E^{(k)}_j
  - \log Z(\beta_k)
\right]
+\mathrm{const.}
\notag \\
&=
\sum_{k = 1}^K
\sum_{j = 1}^{N_k}
\log g( E^{(k)}_j )
-
\sum_{k = 1}^K
N_k \,
\log Z(\beta_k)
+\mathrm{const.}
\end{align}
where
we have used Eq. \eqref{eq:Pg_cond}
on the second line;
on the next line,
we have further removed
the term, $-\beta \, E^{(k)}_j$,
that is independent of $g(E)$.
%
Note that
$Z(\beta_k)$ is
itself a functional of $g(E)$
by Eq. \eqref{eq:Z_def},
so it must be kept.


Now let us take the functional derivative.
%
\begin{align}
\frac{ \delta \LL[g(E)] }
     { \delta g(E) }
&=
\sum_{k = 1}^K
\sum_{j = 1}^{N_k}
\frac{ \delta_{E, \, E^{(k)}_j} }
     { g( E ) }
-
\sum_{k = 1}^K
N_k
\frac{ \exp(-\beta_k \, E) }
     { Z(\beta_k) },
\end{align}
where,
we have used Eqs.
\eqref{eq:funcdiff_logF},
\eqref{eq:funcdiff_g},
and
\eqref{eq:funcdiff_Z}.




Further,
we observe the sum
\[
  \sum_{j = 1}^{N_k} \delta_{E, \, E^{(k)}_j}
\]
gives the number of frames in the $k$th trajectory
with energy equal to $E$,
so it must be equal to $n_k(E)$.
%
Thus,
\begin{align}
\frac{ \delta \LL[g(E)] }
     { \delta g(E) }
=
\frac{  \sum_{k = 1}^K n_k(E) }
     { g( E ) }
-
\sum_{k = 1}^K
N_k
\frac{ \exp(-\beta_k \, E) }
     { Z(\beta_k) }.
\end{align}
%
Around the peak of the likelihood function,
this functional derivation must vanish:
$\delta \LL[g(E)]/ \delta g(E) = 0$.
%
Thus,
we recover Eq. \eqref{eq:gE_WHAM}.




\section{\label{sec:WHAM_var}
Deriving WHAM from the variance analysis}




We have seen two derivations of the WHAM equation.
%
However, the traditional derivation of WHAM\cite{
ferrenberg1988, *ferrenberg1989, kumar1992,
newman, frenkel}
follows a different path
based on the minimization of the variance.
%
Personally,
I think that the derivation
is not as clean as the previous two.
%
Nonetheless,
we shall study it anyway
because of its historical importance.



\subsection{Basic idea}


The basic idea is that
for each simulation,
we have an estimate of the density of states
from Eq. \eqref{eq:gE_onehistogram}.
%
These estimates of $g(E)$
use limited data, and thus have larger errors.
%
An optimal combination of the estimates, however,
should be able to improve the result.


To put it formally,
for each temperature, we have
%
\begin{equation}
\hat g_k(E)
=
\frac{ n_k(E) }
     { d_k(E) }.
\label{eq:gkE_onehistogram}
\end{equation}
where
\begin{equation}
  d_k(E) \equiv N_k \, \frac{ \exp(-\beta_k E) } { Z(\beta_k) },
  \label{eq:dkE_def}
\end{equation}
%
We seek an optimal combination
%
\begin{equation}
g(E)
=
\sum_{k = 1}^K c_k \, \hat g_k(E).
\end{equation}
with the coefficients
under the constraint
$\sum_{k = 1}^K c_k = 1$.


We should note that
although Eq. \eqref{eq:gkE_onehistogram}
gives only an estimate of $g(E)$,
the average value is exact:
\begin{equation}
g(E)
=
\frac{ \langle n_k(E) \rangle}
     { d_k(E) }.
\label{eq:gkE_ave}
\end{equation}



\subsection{Variance analysis}



Let us first study a general theorem
of combining data.
%
Suppose we have $K$ random variables,
$x_1$, \dots, $x_K$,
which are estimates of the same quantity.
%
We wish to linearly combine them to
form an estimate with the smallest error.
%
Below, we shall show that
in an optimal combination,
the weight is inversely proportional to the variance.



The combination is
\[
  X = c_1 \, x_1 + \cdots + c_K \, x_K,
\]
with
\begin{equation}
  c_1 + \cdots + c_K = 1,
  \label{eq:constraint_normalization}
\end{equation}
%
The variance of $X$ is
%
\begin{align*}
  \mathrm{var}(X)
=
\sum_{k = 1}^K c_k^2 \, \mathrm{var}(x_k)
\end{align*}
%
To minimize $\mathrm{var}(X)$,
we seek the minimum
under the Lagrange multiplier,
$\lambda$ of Eq. \eqref{eq:constraint_normalization}.
%
This means minimizing
%
\begin{equation}
F(c_1, \dots c_K)
=
\sum_{k = 1}^K c_k^2 \, \mathrm{var}(x_k)
-\lambda \, \sum_{k = 1}^K c_k.
\end{equation}
%
Now
setting the partial derivatives to zeros as
$\partial F/\partial c_k = 0$
yields,
%
\begin{align*}
  2 \, c_k \, \mathrm{var}(x_k) - \lambda = 0
\end{align*}
%
which shows that
\begin{align*}
c_k
&= \frac{ \lambda/2 }{\mathrm{var}(x_k) }
\propto \frac{ 1 } { \mathrm{var}(x_k) }.
\end{align*}
In other words,
the optimal weight
is inversely proportional to the variance.



\subsection{WHAM}



Under WHAM,
the variance of $\hat g_k(E)$ can be computed as follows.
%
From Eq. \eqref{eq:gkE_onehistogram},
we have
%
\begin{align*}
\var[ \hat g_k(E) ]
&=
\frac{ \var[ n_k(E) ] }
{ [ d_k(E) ]^2 }
\approx
\frac{ \langle n_k(E) \rangle }
{ [ d_k(E) ]^2 }
=
\frac{ g(E) } { d_k(E) }
\propto
\frac{ 1 } { d_k(E) }.
\end{align*}
where we have assumed
$\var[ n(E) ] = \langle n(E) \rangle$
(see Appendix \ref{sec:varhist}
for a derivation),
and used Eq. \eqref{eq:gkE_ave}.



This shows that the optimal weight
of combining $\hat g_k(E)$ is proportional to $d_k(E)$.
%
So
\begin{align}
\hat g(E)
&=
\frac{ \sum_{k = 1}^K d_k(E) \, \hat g_k(E) }
     { \sum_{k = 1}^K d_k(E) }
\notag \\
&=
\frac{ \sum_{k = 1}^K n_k(E) }
     { \sum_{k = 1}^K d_k(E) }.
\label{eq:gE_WHAM_nkdk}
\end{align}
%
Upon expanding $d_k(E)$,
we recover Eq. \eqref{eq:gE_WHAM}.




\section{\label{sec:ST-WHAM}
Statistical-temperature WHAM (ST-WHAM)}



As we have seen,
the WHAM equation requires an iteratively solution.
%
It is possible to avoid the iteration process
with minimal approximation.
%
This iteration-free variant of WHAM
is ST-WHAM\cite{fenwick2008, kim2011}.



We start from Eq. \eqref{eq:gE_WHAM}
or the form by Eq. \eqref{eq:gE_WHAM_nkdk}.
%
We observe that from Eq. \eqref{eq:dkE_def}
the partition function
dependence only lies in the denominator
\[
D(E)
\equiv
\sum_{k = 1}^K d_k(E).
\]
Now, from Eq. \eqref{eq:dkE_def} we have
\[
d_k'(E)
= -\beta_k \, d_k(E).
\]
So
\[
D'(E)
= -\sum_{k = 1}^K \beta_k \, d_k(E),
\]
and
\begin{align}
\frac{d}{dE} \log D(E)
=
\frac{ D'(E) } { D(E) }
=
-\frac{ \sum_{k = 1}^K \beta_k \, d_k(E) }
      { \sum_{k = 1}^K d_k(E) }.
\label{eq:dlogD_step1}
\end{align}



Now our key approximation
is to substitute $n_k(E)$ for $\langle n_k(E) \rangle$
in Eq. \eqref{eq:gkE_ave}:
%
\begin{equation}
  d_k(E)
  =
  \frac{ \langle n_k(E) \rangle } { g(E) }
  \approx
  \frac{ n_k(E) } { g(E) }.
  \label{eq:dkE_approx}
\end{equation}
%
Using this in Eq. \eqref{eq:dlogD_step1} yields
%
\begin{align*}
\frac{d}{dE} \log D(E)
\approx
-\frac{ \sum_{k = 1}^K \beta_k \, n_k(E) / g(E) }
{ \sum_{k = 1}^K n_k(E) / g(E) }
=
-\frac{ \sum_{k = 1}^K \beta_k \, n_k(E) }
{ \sum_{k = 1}^K n_k(E) }.
\end{align*}
%
Notice that the right-hand side
is now free of $d_k(E)$, hence the partition function.
%
Integrating over $E$ yields
%
\begin{align*}
\log D(E)
\approx
-\int^E_{E_{\min}} \frac{ \sum_{k = 1}^K \beta_k \, n_k(E') }
{ \sum_{k = 1}^K n_k(E') } dE'.
\end{align*}
%
Using this in Eq. \eqref{eq:gE_WHAM_nkdk},
we reach the ST-WHAM result
\begin{align}
\hat g(E)
\approx
\left[
  \sum_{k = 1}^K n_k(E)
\right]
\,
\exp
\left[
\int^E_{E_{\min}}
    \frac{ \sum_{k = 1}^K \beta_k \, n_k(E') }
         { \sum_{k = 1}^K n_k(E') }
  dE'
\right].
\label{eq:g_STWHAM}
\end{align}
%
Again, this forms requires
only histograms
and hence is iteration-free.






\appendix



\section{\label{sec:varhist}
Variance of the histogram}



Here we show how to compute
the variance of a histogram.
%
We shall show,
in terms of an energy histogram,
\begin{equation}
  \var[ n(E) ] = \langle n(E) \rangle -  \langle n(E) \rangle^2/N,
  \label{eq:varn}
\end{equation}
with $N$ being the sample size.
%
The second term on the right-hand side
results from the constraint $\sum_E n(E) = N$.
%
If $\langle n(E) \rangle \ll N$,
then we have
\[
  \var[ n(E) ] \approx \langle n(E) \rangle.
\]




To derive Eq. \eqref{eq:varn},
let us consider flipping an unfair coin,
with probability $p$ of getting a head.
%
Then, after $N$ independent flips,
the number of heads $n$ satisfies the binomial distribution.
%
\begin{equation}
  p(n) = {N \choose n} \, p^n \, (1 - p)^{N - n}.
  \label{eq:binomial}
\end{equation}
%
The mean and variance of $n$ are
%
\begin{align}
  \langle n \rangle &= p \, N, \\
  \var(n) &= p \, (1 - p) \, N,
  \label{eq:binomial_moments}
\end{align}
respectively.



Now let us reinterpret $p$ as
the probability of a random data point
assuming energy $E$ (instead of getting a head in the coin flipping),
then the histogram $n(E)$ is the number
of data points assuming $E$ after $N$ independent trials.
%
Thus,
%
\begin{align*}
\langle n \rangle
&=
N \, p,
\\
\var(n)
&=
N \, p \, (1 - p),
=
\langle n \rangle \cdot ( 1 - \langle n \rangle/N).
\end{align*}










\bibliography{../simul}
\end{document}
